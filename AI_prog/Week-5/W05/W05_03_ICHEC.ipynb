{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=img/MScAI_brand.png width=70%></center>\n",
    "\n",
    "# ICHEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=https://www.ichec.ie/sites/default/files/logo.png style=\"background-color:green;\" width=50%></center>\n",
    "\n",
    "ICHEC is the Irish Centre for High-End Computing, a department of NUI Galway hosted in the IT Building. The server farm is in Waterford and some staff are in Dublin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The **Kay** supercomputer\n",
    "* 336 nodes each of 2x 20-core 2.4GHz and 192 GiB of RAM\n",
    "* 16 GPU nodes, as above plus 2x NVIDIA Tesla V100\n",
    "* Some more nodes with bigger processors/more memory\n",
    "* Login nodes and other service nodes.\n",
    "\n",
    "* https://www.ichec.ie/about/infrastructure/kay\n",
    "* https://www.ichec.ie/academic/national-hpc/documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Condominium access\n",
    "\n",
    "Each Irish institution has \"condominium\" access to ICHEC, including MSc students. https://www.ichec.ie/academic/condonium-service\n",
    "\n",
    "To get access:\n",
    "\n",
    "* Register on https://register.ichec.ie/register\n",
    "* Fill in the Word form (in Blackboard) and send to Noreen Goggin <noreen.goggin@nuigalway.ie>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `ssh` \n",
    "\n",
    "`ssh` is a command for logging-in to a remote node. When you have an ICHEC account, you can run this on your laptop:\n",
    "\n",
    "```bash\n",
    "$ ssh my_username@kay.ichec.ie\n",
    "```\n",
    "\n",
    "You'll be asked for your password and you'll end up at a Linux prompt on a `kay` login node. The login node is for managing your files and submitting batch jobs. Do not directly **run** any large jobs there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Access is from within the NUI Galway network only!\n",
    "\n",
    "Later on, for convenience, you can set up access from home/elsewhere also by generating an SSH keypair: https://www.ichec.ie/academic/national-hpc/documentation/ssh-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Python on ICHEC\n",
    "\n",
    "\n",
    "Lots of people are using Python for jobs on ICHEC.\n",
    "\n",
    "https://www.ichec.ie/academic/national-hpc-service/software/python-conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Virtual environments\n",
    "\n",
    "It's recommend to use a **virtual environment** to manage your Python/Anaconda installation. A `venv` is just a collection of software installed with Anaconda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can create multiple virtual environments and activate the one you want. One advantage is you might have one project that uses Tensorflow 1 and another that uses Tensorflow 2, and you want to be able to run both projects at different times. You can create a `venv` for each project with the right packages installed and just switch between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Setting up your environment\n",
    "\n",
    "You can do this on a login node:\n",
    "\n",
    "```sh\n",
    "# tell kay we will use Anaconda\n",
    "module load conda/2\n",
    "# create a new venv\n",
    "conda create --name my_env\n",
    "# activate it\n",
    "conda activate my_env\n",
    "# install the stuff we need \n",
    "conda install tensorflow-gpu networkx scikit-learn\n",
    "# for completeness: exit the virtual environment\n",
    "conda deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A `venv` is just a collection of software installed with Anaconda. It doesn't include the contents of your home directory. So, the next step is to make and/or copy your work directories to `kay`, e.g. on a login node:\n",
    "\n",
    "```bash\n",
    "mkdir work\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You'll need to be able to navigate a Unix filesystem, e.g.:\n",
    "\n",
    "* `mkdir # make directory`\n",
    "* `mv    # move/rename`\n",
    "* `cp    # copy`\n",
    "* `cd    # change directory`\n",
    "* `ls    # list directory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `scp`\n",
    "\n",
    "`scp` is a command for copying files to a remote computer. When you have an ICHEC account, you can run this on your laptop (notice `-r` is for \"recursive copy\"):\n",
    "\n",
    "```bash\n",
    "$ scp -r my_project_directory my_username@kay.ichec.ie:/ichec/home/users/my_username/work/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### NFS\n",
    "\n",
    "The contents of your home directory on the login node (which we started to create above) are synced back and forth to any compute node on `kay` your jobs might run on via `NFS`, the network filesystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interactive testing with `srun`\n",
    "\n",
    "Then you can request a node for eg 30m interactive testing. This is just for testing/prototyping, not for large runs.\n",
    "\n",
    "Run this from the login node:\n",
    "```bash\n",
    "srun -p DevQ -N 1 -A nuig02 -t 0:30:00\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If one is available, you will be delivered to a compute node on DevQ. Then:\n",
    "```bash\n",
    "module load conda/2\n",
    "conda info --envs\n",
    "source activate my_env\n",
    "cd work/my_project_directory\n",
    "python my_prog.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After whatever length of time you requested (e.g. 30m above), your session will expire and you'll be logged out from that node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Batch mode\n",
    "\n",
    "Above, we used `kay` interactively (typing commands at the shell), but **batch mode** is the common way to run large jobs. \"Batch mode\" is the opposite of \"interactive mode\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That sounds like a downgrade! But HPC systems provide batch mode because:\n",
    "\n",
    "* jobs are long-running (hours or days), so no benefit to being interactive;\n",
    "* the system can balance resource allocation for fairness and throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SLURM\n",
    "\n",
    "SLURM is a workload manager. It is the mechanism we use to submit batch jobs for execution on `kay`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src=img/SLURM.png width=55%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Queues\n",
    "When you submit a job for batch execution, you submit it to a **queue**. ICHEC has several distinct queues with different types of nodes, including:\n",
    "\n",
    "Name | Node type | Max nodes | Max walltime | Purpose\n",
    "-----|-----------|-----------|--------------|--------\n",
    "DevQ | Cluster   | \t4        | \t1 hour      | Prototyping\n",
    "ProdQ| Cluster   | \t40       | \t72 hours    | Large jobs\n",
    "GpuQ | GPU       |  4        |  48 hours    | GPU jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Submitting jobs with `sbatch`\n",
    "\n",
    "To specify a job, we first create a special shell script with the `.sh` suffix, e.g. `work/my_project_directory/my_batch_job.sh`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font size=5>\n",
    "    \n",
    "```bash\n",
    "#!/bin/sh\n",
    "\n",
    "# request 1 node for 1 hour on DevQ queue\n",
    "#SBATCH --time=1:00:00\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -p DevQ\n",
    "#SBATCH -A nuig02\n",
    "#SBATCH --mail-user=my.email@nuigalway.ie\n",
    "#SBATCH --mail-type=BEGIN,END\n",
    "\n",
    "module load taskfarm\n",
    "module load conda/2\n",
    "# module load cuda/10 # uncomment for GPU\n",
    "source activate my_env\n",
    "\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "python my_prog.py\n",
    "```\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first line is a standard way of telling the operating system that it is a shell script to be run by `/bin/sh`, a standard shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`sbatch` is the Unix command which submits a job to a queue. We run it like this on a login node:\n",
    "\n",
    "```bash\n",
    "cd work/my_project_directory\n",
    "sbatch my_batch_job.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Taskfarming\n",
    "\n",
    "The above procedure only makes sense if `my_prog.py` is a very large job that requires a whole node to itself!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An easier way to use supercomputer facilities is to run many runs in parallel, one per core. Suppose our program `my_prog.py` has a hyperparameter `n` which takes on values 0-9, and for each we want to run 100 repetitions with 100 random seeds. We write our program to accept arguments `my_prog.py <n> <seed>`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then we can just put these commands into a file named e.g. `tasks.sh`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# each line is one command\n",
    "python my_prog.py 0 0\n",
    "python my_prog.py 0 1\n",
    "python my_prog.py 0 2\n",
    "...\n",
    "python my_prog.py 9 99\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, instead of `python my_prog.py` at the bottom of `my_batch_job.sh`, we'll put a `taskfarm` command:\n",
    "\n",
    "```bash\n",
    "taskfarm tasks.sh\n",
    "```\n",
    "It will take each line of `tasks.sh` as a task. It will start tasks, usually one per core, and when they finish it will start new ones, until the whole file is finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### After submitting\n",
    "\n",
    "* You can check on your queued jobs: `squeue -u my_username`\n",
    "* You should receive an email when the job **starts**. \n",
    "  * It might contain error messages. To debug, check your account name and check that your tasks run ok interactively.\n",
    "* When your job finishes, you should receive another email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GPU on `kay`\n",
    "\n",
    "Ensure that your `venv` includes (e.g.) `tensorflow-gpu` (it should as we installed it earlier). Then request a GPU node for interactive testing:\n",
    "\n",
    "```bash\n",
    "srun -p GpuQ -N 1 -A nuig02 -t 0:30:00\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If one is available, you will be delivered to a compute node with GPU. Then run these:\n",
    "```bash\n",
    "module load cuda/10.0\n",
    "module load conda/2\n",
    "conda info --envs # what envs exist?\n",
    "source activate my_env\n",
    "# test that GPU support is working - you should see lots of messages\n",
    "# including \"name: Tesla V100-PCIE-16GB\"\n",
    "python -c 'import tensorflow as tf; tf.test.gpu_device_name()'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then make sure to include the line `module load cuda/10.0` in your `my_batch_job.sh` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the **Deep Learning** module (in Semester 2 for full-time students, in Year 2 Semester 2 for part-time), you'll have opportunities to use deep neural network libraries such as Tensorflow or PyTorch on GPU."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
