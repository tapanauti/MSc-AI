{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=img/MScAI_brand.png width=70%></center>\n",
    "\n",
    "# Scikit-Learn: Mimicking the Estimator API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we have seen, Scikit-Learn has some infrastructure which helps to make ML projects run smoothly in practice, e.g.:\n",
    "\n",
    "* Appropriate `score` methods for various models;\n",
    "* Cross-validation e.g. `cross_validate_score`;\n",
    "* Pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If we are writing a new ML algorithm, it is natural to try to conform to the Scikit-Learn API so that our code will work well with this infrastructure. For example, if we conform to the Estimator API, then we can put our new algorithm in a list with other models and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "class FabNewClassifier: \n",
    "    pass # whatever\n",
    "clfs = [LogisticRegression, SVC, FabNewClassifier]\n",
    "for clf in clfs:\n",
    "    clf = clf()\n",
    "    clf.fit(X, y)\n",
    "    print(clf.score(X, y))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To make this concrete, we're going to: \n",
    "\n",
    "1. Invent a new classifier and implement it in Python with Numpy. \n",
    "2. Re-factor it as a class with `fit`, `predict`, and `score` methods, which allows the above polymorphism to work.\n",
    "3. *Inherit* from Scikit-Learn base classes to get uniformity and to get functionality \"for free\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "X = iris.drop(\"species\", axis=1).values\n",
    "y = iris[\"species\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The classifier we'll implement is $1$-nearest neighbours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def one_nn(X, y, Q):\n",
    "    D = scipy.spatial.distance.cdist(X, [Q])\n",
    "    nearest = np.argmin(D)\n",
    "    print(\"Query\", Q)\n",
    "    print(\"nearest\", nearest)\n",
    "    print(\"X[nearest]\", X[nearest])\n",
    "    print(\"D[nearest]\", D[nearest])\n",
    "    print(\"y[nearest]\", y[nearest])\n",
    "    return y[nearest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can write it as a single function because $k$-NN has no separate training phase. We use `scipy.spatial.distance.cdist` to calculate all distances from the query `Q` to each point in the training data `X`. Then we return the `y` label of whichever point in `X` is nearest to `Q`. We print out everything that's happening just to help explain, but of course in real code it should just return a value, not print anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query [5.75, 2.0, 4.0, 1.5]\n",
      "nearest 53\n",
      "X[nearest] [5.5 2.3 4.  1.3]\n",
      "D[nearest] [0.43874822]\n",
      "y[nearest] versicolor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'versicolor'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = [5.75, 2.0, 4.0, 1.5]\n",
    "one_nn(X, y, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Refactoring to mimic the Estimator API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know enough to refactor our $1$-NN as an Estimator class. We have to provide:\n",
    "\n",
    "* `fit(X, y)`\n",
    "* `predict(X)`\n",
    "* `score(X, y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class OneNN:\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def predict(self, X):\n",
    "        D = scipy.spatial.distance.cdist(self.X, X)\n",
    "        nearest = np.argmin(D, axis=0)\n",
    "        return self.y[nearest]\n",
    "    def score(self, X, y):\n",
    "        fX = self.predict(X)\n",
    "        return np.mean(fX == y) # accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`OneNN` is now an object. `fit` stores the `X` and `y` but doesn't actually *do* anything for $1$-NN. \n",
    "\n",
    "The biggest change here is that in Scikit-Learn, `predict` should accept a 2D array of query points, not a single point. So we've added `axis=0` to take account of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['versicolor', 'versicolor', 'versicolor'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onenn = OneNN()\n",
    "onenn.fit(X, y)\n",
    "Q3 = np.array([[5.75,  2,   4, 1.5],\n",
    "               [5.0, 2.5, 3.4, 1.6],\n",
    "               [4.6, 2.8, 2.2, 1.7]])\n",
    "onenn.predict(Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And our `score` method calculates an accuracy value. E.g. in $1$-NN we will always get accuracy of 1 on training data. (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(onenn.score(X[:3], y[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Estimator API and inheritance\n",
    "\n",
    "However, our `OneNN` is still not quite compatible with Scikit-Learn. There are some extra details which help to keep things uniform and make our lives easier. For a start, estimators should inherit from `sklearn.base.BaseEstimator`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "class OneNN(BaseEstimator, ClassifierMixin):\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        D = scipy.spatial.distance.cdist(self.X, X)\n",
    "        nearest = np.argmin(D, axis=0)\n",
    "        return self.y[nearest]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A *Mixin* is a class M which is designed for the *multiple inheritance* scenario where a class C is designed to inherit from M and from some other class D as well. In other words, C is composed of M \"mixed-in\" with D.\n",
    "\n",
    "In Scikit-Learn, there is (e.g.) a `ClassifierMixin` which has the `score` behaviour. This makes sense, because all classifiers should share the same `score` behaviour: our `OneNN` class should not implement a custom version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['versicolor', 'virginica', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q3 = np.array([[5.75,  2,   4, 1.5],\n",
    "               [5.0, 1.5, 2.4, 1.6],\n",
    "               [4.6, 2.8, 2.2, 1.7]])\n",
    "\n",
    "onenn = OneNN().fit(X, y)\n",
    "onenn.predict(Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onenn.score(X[:3], y[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More details of the API\n",
    "\n",
    "Here is a flavour of the \"rules\" of the API which are designed to keep things uniform and make both users and developers' lives easier.\n",
    "\n",
    "* The arguments of `__init__` should be keyword arguments with defaults. So, calling `C()` (no arguments) will work.\n",
    "* In order to fit in pipelines, even unsupervised estimators need to accept `y=None`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The `fit` method should return `self`. This allows a nice \"chained\" usage `OneNN().fit(X, y).predict(Q3)`\n",
    "* \"In iterative algorithms, the number of iterations should be specified by an integer called `n_iter`.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* \"You can check whether your estimator adheres to the scikit-learn interface and standards by running `utils.estimator_checks.check_estimator` on the class\"\n",
    "* There is a template for new \"contrib\" projects: https://github.com/scikit-learn-contrib/project-template/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* https://scikit-learn.org/dev/developers/develop.html\n",
    "* https://scikit-learn.org/stable/modules/classes.html#module-sklearn.base"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
