{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=img/MScAI_brand.png width=70%></center>\n",
    "\n",
    "# Scikit-Learn: Feature Selection/Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Feature selection: we choose a subset of existing features\n",
    "* Feature engineering: we construct new features from existing data\n",
    "\n",
    "In Scikit-Learn, both are implemented as *transformers*: a `transform(X)` method, usually preceded by `fit(X)`. And optionally `fit_transform(X)` as a shortcut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature selection\n",
    "\n",
    "* **Filter**: calculate a statistic per feature and choose those above a threshold.\n",
    "* **Wrapper**: try different subsets and see which gives best performance\n",
    "\n",
    "Reference:\n",
    "* https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filter approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 1. ]\n",
      " [0.3 1. ]\n",
      " [0.1 1. ]\n",
      " [0.9 1. ]\n",
      " [0.8 1. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = np.array([[0.5, 1.0], [0.3, 1.0], [0.1, 1.0], \n",
    "              [0.9, 1.0], [0.8, 1.0]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`VarianceThreshold` throws away features with too little variance. By default, only zero variance is thrown away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5],\n",
       "       [0.3],\n",
       "       [0.1],\n",
       "       [0.9],\n",
       "       [0.8]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = VarianceThreshold() \n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *chi-squared* ($\\chi^2$) score is a measure of correlation between a numerical feature and a discrete target. \n",
    "\n",
    "<center><img src=img/feature_selection.svg width=55%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Higher values indicate more informative features. So, it's natural to throw away low-valued features. Conceptually, we sort and then impose a threshold, chosen to keep the best `k` features.\n",
    "\n",
    "\n",
    "<center><img src=img/feature_selection2.svg width=55%></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores [ 10.81782088   3.7107283  116.31261309  67.0483602 ]\n",
      "Shapes (150, 4) (150, 2)\n"
     ]
    }
   ],
   "source": [
    "sel = SelectKBest(chi2, k=2).fit(X, y)\n",
    "X_new = sel.transform(X)\n",
    "print(\"Scores\", sel.scores_)\n",
    "print(\"Shapes\", X.shape, X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature selection\n",
    "\n",
    "Two main approaches:\n",
    "\n",
    "* **Filter**: calculate a statistic per feature and choose those above a threshold (**done**)\n",
    "* **Wrapper**: try different subsets and see which gives best performance (**see exercise**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature engineering\n",
    "\n",
    "* Scaling\n",
    "* Missing values\n",
    "* One-hot encoding\n",
    "* Arithmetic feature transformations\n",
    "* Text features\n",
    "* Image features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scaling\n",
    "\n",
    "Some ML methods work better if features are normalised to $[0, 1]$ or standardised to have mean 0 and standard deviation 1. Scikit-Learn provides `StandardScaler`, for example, for the latter. The calculation is simple: $(X - \\bar{X}) / \\sigma(X)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=img/data-leak.jpg width=15%><font size=1><a href=\"https://meterpreter.org/us-postal-service-website-vulnerability-leaked-60-million/\">meterpreter.org</a></font></center>\n",
    "\n",
    "A slight complication is the rule that we must not leak information about the test set into our training procedure. So, we first calculate the mean and std of the train set, and use them to transform the train set. We then use the same values to transform the test set. We never calculate the mean and standard deviation of the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = np.array([0, 4, 1, 6, 7, 8, 5, 9.0]\n",
    "                  ).reshape(-1, 1)\n",
    "X_test = np.array([3.3, 4.5, 5.5]\n",
    "                 ).reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# do not fit on X_test!\n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a result, `X_train` is now standardised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.66666667],\n",
       "       [-0.33333333],\n",
       "       [-1.33333333],\n",
       "       [ 0.33333333],\n",
       "       [ 0.66666667],\n",
       "       [ 1.        ],\n",
       "       [ 0.        ],\n",
       "       [ 1.33333333]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But `X_test` will not have a zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.56666667],\n",
       "       [-0.16666667],\n",
       "       [ 0.16666667]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Imputing missing values\n",
    "\n",
    "It's common to have missing values in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 4.],\n",
       "       [ 1.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       [nan],\n",
       "       [ 5.],\n",
       "       [ 9.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([0, 4, 1, 6, 7, np.nan, 5, 9.0]\n",
    "            ).reshape(-1, 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A common strategy is just to impute the mean of the values present in the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [4.        ],\n",
       "       [1.        ],\n",
       "       [6.        ],\n",
       "       [7.        ],\n",
       "       [4.57142857],\n",
       "       [5.        ],\n",
       "       [9.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "X2 = imp.fit_transform(X)\n",
    "X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Arithmetic feature transformations\n",
    "\n",
    "<center><img src=img/arithmetic-feature-transformation.png width=50%></center> \n",
    "<font size=2>Derived from PDSH; code in `code/make_arithmetic_transformation_plot.py`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose we have data like the above. We'll find that linear regression $y = a+bx$ doesn't model it well (left). But if we added the feature $x^2$ to give the model $y = a+bx+b_2x^2$, we could find a good fit (right)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The same idea can in principle work for $x^3$ and higher. These are called *polynomial features*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.      0.      0.   ]\n",
      " [  1.5     2.25    3.375]\n",
      " [  2.      4.      8.   ]\n",
      " [  4.     16.     64.   ]\n",
      " [  4.5    20.25   91.125]\n",
      " [  5.     25.    125.   ]\n",
      " [  6.     36.    216.   ]\n",
      " [  7.     49.    343.   ]\n",
      " [  8.     64.    512.   ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = np.array([0, 1.5, 2, 4, 4.5, 5, 6, 7, 8]\n",
    "            ).reshape(-1, 1)\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X2 = poly.fit_transform(X)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### One-hot Encoding\n",
    "\n",
    "In *one-hot encoding*, we convert a single categorical feature `f` with $n$ levels to $n$ binary features `f0`, `f1`, etc:\n",
    "\n",
    "`f` | `f0` | `f1` | `f2`\n",
    "----|------|------|-----\n",
    " `a`|  1   |   0  |  0\n",
    " `b`|  0   |   1  |  0 \n",
    " `a`|  1   |   0  |  0\n",
    " `c`|  0   |   0  |  1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Of course, Scikit-Learn provides that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "f = [[\"a\"], [\"b\"], [\"a\"], [\"c\"]]\n",
    "OneHotEncoder(sparse=False).fit_transform(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `DictVectorizer`\n",
    "\n",
    "`DictVectorizer` converts a dataset in the form of a list of dicts to a nice rectangular array. It does one-hot encoding on any fields which need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PDSH \n",
    "# https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html\n",
    "data = [ \n",
    "    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},\n",
    "    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},\n",
    "    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},\n",
    "    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LabelEncoder in module sklearn.preprocessing.label:\n",
      "\n",
      "class LabelEncoder(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      " |  Encode labels with value between 0 and n_classes-1.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_targets>`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : array of shape (n_class,)\n",
      " |      Holds the label for each class.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  `LabelEncoder` can be used to normalize labels.\n",
      " |  \n",
      " |  >>> from sklearn import preprocessing\n",
      " |  >>> le = preprocessing.LabelEncoder()\n",
      " |  >>> le.fit([1, 2, 2, 6])\n",
      " |  LabelEncoder()\n",
      " |  >>> le.classes_\n",
      " |  array([1, 2, 6])\n",
      " |  >>> le.transform([1, 1, 2, 6]) #doctest: +ELLIPSIS\n",
      " |  array([0, 0, 1, 2]...)\n",
      " |  >>> le.inverse_transform([0, 0, 1, 2])\n",
      " |  array([1, 1, 2, 6])\n",
      " |  \n",
      " |  It can also be used to transform non-numerical labels (as long as they are\n",
      " |  hashable and comparable) to numerical labels.\n",
      " |  \n",
      " |  >>> le = preprocessing.LabelEncoder()\n",
      " |  >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
      " |  LabelEncoder()\n",
      " |  >>> list(le.classes_)\n",
      " |  ['amsterdam', 'paris', 'tokyo']\n",
      " |  >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) #doctest: +ELLIPSIS\n",
      " |  array([2, 2, 1]...)\n",
      " |  >>> list(le.inverse_transform([2, 2, 1]))\n",
      " |  ['tokyo', 'tokyo', 'paris']\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.preprocessing.OrdinalEncoder : encode categorical features\n",
      " |      using a one-hot or ordinal encoding scheme.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LabelEncoder\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, y)\n",
      " |      Fit label encoder\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  fit_transform(self, y)\n",
      " |      Fit label encoder and return encoded labels\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape [n_samples]\n",
      " |  \n",
      " |  inverse_transform(self, y)\n",
      " |      Transform labels back to original encoding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : numpy array of shape [n_samples]\n",
      " |  \n",
      " |  transform(self, y)\n",
      " |      Transform labels to normalized encoding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape [n_samples]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer(sparse=False, dtype=int)\n",
    "vec.fit_transform(data)\n",
    "vec.get_feature_names()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "help(LabelEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text features\n",
    "\n",
    "When we have text data, we need even more work to convert to rectangular data. Sophisticated methods are covered in the NLP module. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here we'll see a simple TF-IDF approach based on [PDSH](https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html#Text-Features). \n",
    "\n",
    "It assumes that we have a list of strings, and we want to convert each string to a row in an (unlabelled) rectangular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# from PDSH\n",
    "sample = ['problem of evil',\n",
    "          'evil queen',\n",
    "          'horizon problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evil</th>\n",
       "      <th>horizon</th>\n",
       "      <th>of</th>\n",
       "      <th>problem</th>\n",
       "      <th>queen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.680919</td>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.605349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.795961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.605349</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       evil   horizon        of   problem     queen\n",
       "0  0.517856  0.000000  0.680919  0.517856  0.000000\n",
       "1  0.605349  0.000000  0.000000  0.000000  0.795961\n",
       "2  0.000000  0.795961  0.000000  0.605349  0.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text \\\n",
    "    import TfidfVectorizer\n",
    "import pandas as pd\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(sample) \n",
    "pd.DataFrame(X.toarray(),  \n",
    "             columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Image features\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.image: we will not cover these. It's usually better to just use a convolutional neural network which is outside our scope here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Scikit-Learn gives us lots of methods for feature **selection** and feature **engineering**, and we've seen a wide sample of the most important and simplest ones.\n",
    "\n",
    "A nice thing about the Scikit-Learn API is that the same code can be used for training a model on a dataset, regardless of whether or how we have done feature selection or feature engineering on that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 1. 0. 0.]]\n",
      "['x0_0.1' 'x0_0.2' 'x0_0.3' 'x1_blue' 'x1_green' 'x1_red']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X = np.array([[0.1, \"red\"  ], \n",
    "              [0.2, \"blue\" ], \n",
    "              [0.1, \"green\"], \n",
    "              [0.3, \"blue\" ]])\n",
    "ohe = OneHotEncoder().fit(X)\n",
    "print(ohe.transform(X).toarray())\n",
    "print(ohe.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise 1**. We saw Polynomial features above, but degrees higher than 2 are often hard to justify (multiple \"turns\") and could lead to overfitting. Perhaps more reasonable transformations are things like $\\log(x)$, $e^x$, and $\\sqrt{x}$. To do these transformations, we would use [`FunctionTransformer`](\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer). Look this up and use it run a square-root transform on this data:\n",
    "\n",
    "```python\n",
    "X = np.array([0, 1.5, 2, 4, 4.5, 5, 6, 7, 8]\n",
    "            ).reshape(-1, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise 2** (background). The *wrapper* approach to feature selection is to try different subsets and see which gives best performance when used inside the ML model we want to use them in. There are at least three approaches:\n",
    "\n",
    "* Forward\n",
    "* Backward\n",
    "* Metaheuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the *forward* approach, we start with 0 features, and try adding one at a time, re-training many times.\n",
    "\n",
    "In the *backward* approach, also known as *recursive feature elimination*, we train with all features, and try removing one at a time, re-training many times. One way to decide what to eliminate is to use coefficient (`coef_`) values of fitted regression models. Some decision tree models provide `feature_importances_` and these can be used instead of `coef_`.\n",
    "\n",
    "In the *metaheuristic* approach, we use a search algorithm like a genetic algorithm to try out different subsets. (Full-time MScAI students may study this in CT5141 Optimisation in Semester 2.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise 2**. Run this:\n",
    "```python\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "help(SelectFromModel)\n",
    "```\n",
    "and then implement recursive feature elimination on `iris` using `SelectFromModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise 3**. In the TF-IDF vectorizer, why did we need `X.toarray()`? What was `X` before that? Why does the `TfidfVectorizer` choose to return results in that format?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [2.        ]\n",
      " [1.        ]\n",
      " [2.44948974]\n",
      " [2.64575131]\n",
      " [2.82842712]\n",
      " [2.23606798]\n",
      " [3.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "func_trans = FunctionTransformer(\n",
    "    lambda X: np.sqrt(X), validate=False)\n",
    "X2 = func_trans.fit_transform(X)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, we transformed all of `X`. It's a bit more complicated to transform just one column and leave other columns alone, so we won't cover that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=img/data-leak.jpg width=15%><font size=1><a href=\"https://meterpreter.org/us-postal-service-website-vulnerability-leaked-60-million/\">meterpreter.org</a></font></center>\n",
    "\n",
    "Transformations like one-hot encoding and $x^2$ and $\\sqrt{x}$ are *stateless*, so we don't have to worry about leaking information from the test set into our training, so we can just carry out these transformations on the whole dataset up-front."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "print(X.shape) # check X shape for later comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.00666667 0.78202798 0.21130535]\n",
      "(150, 1)\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeRegressor() # usual workflow\n",
    "clf = clf.fit(X, y)\n",
    "# special attribute available after fitting \n",
    "# see https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "print(clf.feature_importances_) \n",
    "# prefit=True: we have already fit()ted\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "# remember: feature selection/engineering is a transformer\n",
    "X_new = model.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution 3**. `X` alone is a *sparse* matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(sample) \n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is chosen because conceptually, the result of TF-IDF vectorisation is an array of mostly zeros: most sentences contain a very small sample of all possible words, and the TF-IDF value is 0 for a word not present in a sentence. So, we save a huge amount of space by storing in \"Compressed Sparse Row\" format, which just stores the non-zero values. Conceptually that could be a list of tuples:\n",
    "\n",
    "```python\n",
    "[ \n",
    "    (evil, 0, 0.517856),\n",
    "    (horizon, 2, 0.795961),\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or it could be stored as a dict:\n",
    "\n",
    "```python\n",
    "{\n",
    "(evil, 0): 0.517856,\n",
    "(horizon, 2): 0.795961,\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
