{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exercise Sheet 1.0 - Python - Solutions.ipynb","provenance":[],"collapsed_sections":["6I8RGDU2uj6V"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kJY7QD40vrm-"},"source":["# Exercise 0"]},{"cell_type":"markdown","metadata":{"id":"RJYUWq73vsk2"},"source":["The motivation of this exercise is to gain familiarity with the Python programming language. We are going to do some basic text processing and analysis on a plaintext corpus. If you are not with familiar Python or Jupyter notebooks, it is recommended to start with the Python Tutorial notebook before attempting this exercise.\n","\n","---\n","\n","For this exercise, we are going to count the 25 most frequent words in **The Adventures of Sherlock Holmes**, by Sir Arthur Conan Doyle. You are free to use any other piece of text of your choice for this exercise. This notebook contains step by step instructions (with some hints) and you are required to fill in the code blocks based on the material covered in the Python Tutorial notebook."]},{"cell_type":"markdown","metadata":{"id":"T48PL4payl55"},"source":["### 0. Download the text file.\n","Run the cell below to download the book **The Adventures of Sherlock Holmes** as a text file from [Project Gutenberg](http://www.gutenberg.org), and save into a file called `sherlock.txt`."]},{"cell_type":"code","metadata":{"id":"nW1cmwsAfXh9","executionInfo":{"status":"ok","timestamp":1600609981611,"user_tz":-60,"elapsed":1825,"user":{"displayName":"Nivranshu Pasricha","photoUrl":"","userId":"04741348677416923358"}},"outputId":"d3a31ba9-92cb-4a15-f8bf-30651e17f7c4","colab":{"base_uri":"https://localhost:8080/","height":97}},"source":["!curl https://www.gutenberg.org/files/1661/1661-0.txt > sherlock.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  593k  100  593k    0     0  1413k      0 --:--:-- --:--:-- --:--:-- 1413k\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bSRMWLlFfp22"},"source":["---\n","### 1. Read text from file.\n","Open the text file `sherlock.txt` and read all the lines into a list."]},{"cell_type":"code","metadata":{"id":"QOshGZfAfhA1"},"source":["lines = []  # read lines from sherlock.txt into this list\n","with open('sherlock.txt', 'r') as f:\n","  lines = f.readlines()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dt9qAKiVhH2m"},"source":["---\n","### 2. Filter out the metadata.\n","The text file contains some metadata about the book which is not relevant for our analysis. Discard this information by removing the first 32 lines from the beginning and the last 368 lines from the end."]},{"cell_type":"code","metadata":{"id":"YwnNTTxUhFkE"},"source":["lines = lines[32:-368]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MTvlIw-whuwy"},"source":["---\n","### 3. Remove leading and trailing spaces from each line in the list.\n","Each line contains a newline character `\\n` at the end while some lines also contain leading and trailing spaces. This formatting is done for presentation purposes and not relevant for our analysis."]},{"cell_type":"code","metadata":{"id":"UYdh18lHgsj0"},"source":["clean_lines = []  # store the lines in this list after removing the leading and trailing spaces\n","for line in lines:\n","  clean_lines.append(line.strip())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nSu406W9iqMo"},"source":["---\n","### 4. Remove empty lines from the list.\n","After removing the newline character `\\n` from each line in the list, some strings are now empty that can be discarded safely."]},{"cell_type":"code","metadata":{"id":"-1djnrvHh2kc"},"source":["non_empty_lines = []  # store non empty lines in this list\n","for line in clean_lines:\n","  if line != '':\n","    non_empty_lines.append(line)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VNCnBgL-jNe1"},"source":["---\n","### 5. Join all the non empty lines into a single string.\n","Now that we have cleaned the corpus by removing the presentation details, we can focus on the actual text. Create a single string which contains all the lines from the text.\n","\n"]},{"cell_type":"code","metadata":{"id":"iED8UQpciCv7"},"source":["# text = # join all the lines into this string\n","text = ' '.join(non_empty_lines)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vxri7Uw_keFd"},"source":["---\n","### 6. Convert to lowercase\n","To keep the word counts consistent, we are going to covert everything lowercase. If we don't do this, the words, **the** **The** and **THE**, would be considered distinct.  "]},{"cell_type":"code","metadata":{"id":"dy8tLkbqj2mw"},"source":["text = text.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-6jvhDdJki-_"},"source":["---\n","### 7. Get a list of all the words in the text."]},{"cell_type":"code","metadata":{"id":"lyYdI8rrkfLL"},"source":["words = text.split(' ')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ID09CxSky1g"},"source":["---\n","### 8. How many total words are there in the text?"]},{"cell_type":"code","metadata":{"id":"r8UY3Em8kpRb","executionInfo":{"status":"ok","timestamp":1600609226373,"user_tz":-60,"elapsed":643,"user":{"displayName":"Nivranshu Pasricha","photoUrl":"","userId":"04741348677416923358"}},"outputId":"3ed4d620-2cf6-4773-caa5-45c7441eaf41","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(words)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["104541"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"Ome93kmzk2tU"},"source":["---\n","### 9. How many unique words are there in the text?"]},{"cell_type":"code","metadata":{"id":"C2rJYlgekqkL","executionInfo":{"status":"ok","timestamp":1600609227390,"user_tz":-60,"elapsed":418,"user":{"displayName":"Nivranshu Pasricha","photoUrl":"","userId":"04741348677416923358"}},"outputId":"a8706626-cc53-4b0c-cd5d-88941ad180f1","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(set(words))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["14085"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"dnSpE4-elByQ"},"source":["---\n","### 10. What are the 25 most frequent words?"]},{"cell_type":"code","metadata":{"id":"V62gHcL1ufqK","executionInfo":{"status":"ok","timestamp":1600609229887,"user_tz":-60,"elapsed":566,"user":{"displayName":"Nivranshu Pasricha","photoUrl":"","userId":"04741348677416923358"}},"outputId":"6a72a2a0-9dd3-4482-c706-7086941661be","colab":{"base_uri":"https://localhost:8080/","height":442}},"source":["word_counts = dict()    # create an empty dictionary for word counts\n","for word in words:\n","  if word in word_counts:\n","    word_counts[word] += 1\n","  else:\n","    word_counts[word] = 1\n","\n","word_counts = list(word_counts.items())  # convert dict to a list of tuples for word counts\n","sorted_by_word_counts = sorted(word_counts, key=lambda x: x[1], reverse=True)\n","sorted_by_word_counts[:25]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('the', 5519),\n"," ('and', 2812),\n"," ('to', 2643),\n"," ('of', 2633),\n"," ('a', 2592),\n"," ('i', 2533),\n"," ('in', 1701),\n"," ('that', 1590),\n"," ('was', 1369),\n"," ('he', 1276),\n"," ('it', 1255),\n"," ('his', 1146),\n"," ('you', 1103),\n"," ('is', 1057),\n"," ('my', 955),\n"," ('have', 897),\n"," ('as', 839),\n"," ('with', 822),\n"," ('had', 813),\n"," ('at', 755),\n"," ('which', 747),\n"," ('for', 701),\n"," ('be', 596),\n"," ('not', 582),\n"," ('but', 536)]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"6I8RGDU2uj6V"},"source":["#### Alternate Solutions:"]},{"cell_type":"markdown","metadata":{"id":"PWuhpqKsvNOg"},"source":["1. Python >= 3.6 supports ordered dictionaries, so there is no need to convert to a list of tuples before sorting.\n","2. Look up the `Counter` container in the `collections` module in the [Python docs](https://docs.python.org/3/library/collections.html#collections.Counter)."]}]}