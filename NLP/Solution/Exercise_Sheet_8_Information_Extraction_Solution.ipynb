{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise Sheet 8 - Information Extraction - Solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0jKgZZ79OwP"
      },
      "source": [
        "# Exercise Sheet 8 - Information Extraction - Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRi-fFM8vYPv"
      },
      "source": [
        "## Learning Objectives \n",
        "\n",
        "In this lab we are going to:\n",
        "\n",
        "* Review preprocessing techniques for Information Extraction.\n",
        "* Learn how to find useful information from the text.\n",
        "* Play around with a corpus to extract Named-Entities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgBC1FruXlR1"
      },
      "source": [
        "# setting the stage, as usual with colab ;)\n",
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzvGG2luvYPx"
      },
      "source": [
        "## 1. Preprocessing ##\n",
        "\n",
        "To processes a document we will follow  5 main steps: \n",
        "\n",
        "- Sentence segmentation.\n",
        "- Tokenization. \n",
        "- Tag words with their part-of-speech tags. \n",
        "- Identify interesting chunks and entities. \n",
        "- Identify relations between different entities in the text.\n",
        "\n",
        "The first 3 steps perform linguistic preprocessing of a given text in order to perform higher level processing.\n",
        "\n",
        "\n",
        "This lab is partially based on the Information Extraction chapter of the [NLTK book](http://www.nltk.org/book/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ4DM3CuvYPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d60773-2db8-404d-8ac1-a7dedbe0561d"
      },
      "source": [
        "document = 'The fourth Wells account moving to another agency is the packaged paper-products division of \\\n",
        "Georgia-Pacific Corp., which arrived at Wells only last fall. Like Hertz and the History Channel, \\\n",
        "it is also leaving for an Omnicom-owned agency, the BBDO South unit of BBDO Worldwide. BBDO South in Atlanta, \\\n",
        "which handles corporate advertising for Georgia-Pacific, will assume additional duties for brands like Angel\\\n",
        "Soft toilet tissue and Sparkle paper towels, said Ken Haldin, a spokesman for Georgia-Pacific in Atlanta.'\n",
        "\n",
        "\n",
        "## Preprocessing\n",
        "\n",
        "# Step 1: Sentence segmentation.\n",
        "sentences = nltk.sent_tokenize(document)\n",
        "\n",
        "# Step 2: Tokenize sentences into words.\n",
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Step 3: POS tagging.\n",
        "tagged_sentences = [nltk.pos_tag(sent) for sent in tokenized_sentences]\n",
        "\n",
        "\n",
        "# print the first sentence\n",
        "tagged_sentences[0]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('fourth', 'JJ'),\n",
              " ('Wells', 'NNP'),\n",
              " ('account', 'NN'),\n",
              " ('moving', 'VBG'),\n",
              " ('to', 'TO'),\n",
              " ('another', 'DT'),\n",
              " ('agency', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('packaged', 'VBN'),\n",
              " ('paper-products', 'NNS'),\n",
              " ('division', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('Georgia-Pacific', 'NNP'),\n",
              " ('Corp.', 'NNP'),\n",
              " (',', ','),\n",
              " ('which', 'WDT'),\n",
              " ('arrived', 'VBD'),\n",
              " ('at', 'IN'),\n",
              " ('Wells', 'NNP'),\n",
              " ('only', 'RB'),\n",
              " ('last', 'JJ'),\n",
              " ('fall', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FztiPPt-vYP5"
      },
      "source": [
        "## 2. Named Entity Recognition ##\n",
        "\n",
        "Named entities are definite noun phrases that refer to specific types of individuals, such as organizations, persons, dates, and so on. \n",
        "\n",
        "NLTK provides a classifier that has already been trained to recognize named entities, accessed with the function `nltk.ne_chunk()`. \n",
        "If we set the parameter `binary=True`, then named entities are just tagged as NE; otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I9Ox7hdvYP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22ab86d-6ed6-4965-baf8-07c7ad5daf71"
      },
      "source": [
        "sentence = \"I will meet John Smith to visit Oracle headquarters.\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)   # tokenization\n",
        "pos_tags = nltk.pos_tag(tokens)         # pos-tagging\n",
        "\n",
        "# named entity chunking\n",
        "print(nltk.ne_chunk(pos_tags, binary=True))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  I/PRP\n",
            "  will/MD\n",
            "  meet/VB\n",
            "  (NE John/NNP Smith/NNP)\n",
            "  to/TO\n",
            "  visit/VB\n",
            "  (NE Oracle/NNP)\n",
            "  headquarters/NNS\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jm8VZ3tMNWb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84a8950d-7b1e-49cf-e7e4-64c50253505d"
      },
      "source": [
        "tree = nltk.ne_chunk(pos_tags, binary=True)\n",
        "\n",
        "# find named entities\n",
        "named_entities = []\n",
        "\n",
        "for subtree in tree.subtrees():\n",
        "  if subtree.label() == 'NE':\n",
        "    entity = \"\"\n",
        "    for leaf in subtree.leaves():\n",
        "      entity = entity + leaf[0] + \" \"\n",
        "    named_entities.append(entity.strip())\n",
        "             \n",
        "print(named_entities)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['John Smith', 'Oracle']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_hM50x4vYQE"
      },
      "source": [
        "### Exercise 1 ###\n",
        "\n",
        "Extract all the named entities from the first 20 sentences of the of the Brown Corpus that are in the category '<i>news</i>'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xqzx423OvYQG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f53c731-fe27-44cd-ac69-417739dd685c"
      },
      "source": [
        "# your code goes here\n",
        "\n",
        "def extract_entity_names(tree):\n",
        "  named_entities = []\n",
        "  for subtree in tree.subtrees():\n",
        "    if subtree.label() == 'NE':\n",
        "      entity = \"\"\n",
        "      for leaf in subtree.leaves():\n",
        "        entity = entity + leaf[0] + \" \"\n",
        "      named_entities.append(entity.strip())\n",
        "  return named_entities\n",
        "\n",
        "sentences = nltk.corpus.brown.sents(categories=['news'])[:20]\n",
        "\n",
        "tagged_sentences = nltk.pos_tag_sents(sentences)\n",
        "chunks = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
        "\n",
        "for chunk in chunks:\n",
        "  print(extract_entity_names(chunk))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Fulton County Grand Jury']\n",
            "['City Executive Committee', 'Atlanta']\n",
            "['Fulton Superior Court']\n",
            "[]\n",
            "[]\n",
            "['Fulton']\n",
            "['Atlanta', 'Fulton County']\n",
            "['Merger']\n",
            "[]\n",
            "['City Purchasing Department']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "['Fulton County', 'Fulton County']\n",
            "[]\n",
            "['Fulton County']\n",
            "['Fulton']\n",
            "['Fulton']\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHyERj2JvYQL"
      },
      "source": [
        "## 3. Relation Extraction ##\n",
        "We now want to extract the relations that exist between the specific types of named entities. One way of approaching this task is to use regular expressions to look for all triples of the form (X, α, Y), where X and Y are named entities of the required types, and α is the string of words that intervenes between X and Y.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTOKSyHtvYQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499cf0a0-3f6e-4241-a611-c6c7402d3e71"
      },
      "source": [
        "import re, nltk\n",
        "\n",
        "# Search for strings that contain the word \"in\".\n",
        "\n",
        "# \\b matches the empty string, but only at the beginning or end of a word. (b = boundary)\n",
        "'''Negative lookahead assertion(?<= ...). Matches if ... doesnt match next. \n",
        "To disregard the strings such as success in supervising, where in is followed by a gerund.'''\n",
        "\n",
        "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing\\b)')\n",
        "\n",
        "# Using the documents from the IEEE Corpus - New York Times, 15 March 1998.\n",
        "# (see details here: http://www.nltk.org/_modules/nltk/corpus/reader/ieer.html)\n",
        "docs = nltk.corpus.ieer.parsed_docs('NYT_19980315')\n",
        "\n",
        "for doc in docs:\n",
        "  for rel in nltk.sem.relextract.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern=IN):\n",
        "    print(nltk.sem.relextract.rtuple(rel))\n",
        "    print(nltk.sem.clause(rel, relsym = \"IN\"))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
            "IN('whyy', 'philadelphia')\n",
            "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
            "IN('mcglashan_&_sarrail', 'san_mateo')\n",
            "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
            "IN('freedom_forum', 'arlington')\n",
            "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
            "IN('brookings_institution', 'washington')\n",
            "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
            "IN('idealab', 'los_angeles')\n",
            "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
            "IN('open_text', 'waterloo')\n",
            "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
            "IN('wgbh', 'boston')\n",
            "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
            "IN('bastille_opera', 'paris')\n",
            "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
            "IN('omnicom', 'new_york')\n",
            "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
            "IN('ddb_needham', 'new_york')\n",
            "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
            "IN('kaplan_thaler_group', 'new_york')\n",
            "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
            "IN('bbdo_south', 'atlanta')\n",
            "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n",
            "IN('georgia-pacific', 'atlanta')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YMKuuyOvYQT"
      },
      "source": [
        "### Exercise 2 ###\n",
        "\n",
        "Extract places of birth of people from the the ieeer corpus, using the 'X born in Y' pattern, where X is a person and Y is a location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl9ivjcSvYQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970a6c3f-367c-42ec-b56c-f8effa9ba267"
      },
      "source": [
        "# your code goes here\n",
        "# example of the output \n",
        "# [PER: 'McCarthy'] 'was born in' [LOC: 'Belle Plaine']\n",
        "# Birthplace('mccarthy', 'belle_plaine')\n",
        "\n",
        "from nltk.corpus import ieer\n",
        "\n",
        "BORN_IN = re.compile(r'.*\\bborn in\\b')\n",
        "\n",
        "for fileId in ieer.fileids():\n",
        "  for doc in nltk.corpus.ieer.parsed_docs(fileId):\n",
        "    for rel in nltk.sem.relextract.extract_rels('PER', 'LOC', doc, corpus='ieer', pattern=BORN_IN):\n",
        "      print(nltk.sem.relextract.rtuple(rel))\n",
        "      print (nltk.sem.clause(rel, relsym = \"Birthplace\"))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PER: 'McCarthy'] 'was born in' [LOC: 'Belle Plaine']\n",
            "Birthplace('mccarthy', 'belle_plaine')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5p9fUXpx5z3",
        "outputId": "4c4c06cd-60ad-4106-f87d-973f85ff07ac"
      },
      "source": [
        "# Another example\n",
        "sentences = [\"Barack Hussein Obama II (born August 4, 1961) is an American politician and attorney.\",\n",
        "             \"Obama was born in Honolulu, Hawaii.\",\n",
        "             \"After graduating from Columbia University in 1983, he worked as a community organizer in Chicago.\"]\n",
        "\n",
        "BORN_IN = re.compile(r'.*\\bborn\\b .*')\n",
        "\n",
        "for sent in sentences:\n",
        "  print(re.findall(BORN_IN, sent))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Barack Hussein Obama II (born August 4, 1961) is an American politician and attorney.']\n",
            "['Obama was born in Honolulu, Hawaii.']\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6FnbakKvYQZ"
      },
      "source": [
        "### Exercise 3 ###\n",
        "\n",
        "Extract people and their role in an organisation by using the 'X ROLE at the Y' or 'X ROLE of the Y' patterns, where X is a person and Y is an organisation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9uf32hF4vYQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d617c1-7de6-4209-aa67-091247b4890e"
      },
      "source": [
        "# your code goes here\n",
        "from nltk.corpus import ieer\n",
        "\n",
        "ROLES = re.compile(',.*(\\sat\\sthe?|\\sof\\sthe?)')\n",
        "\n",
        "for file in ieer.fileids():\n",
        "  for doc in nltk.corpus.ieer.parsed_docs(file):\n",
        "    for r in nltk.sem.relextract.extract_rels('PER', 'ORG', doc, corpus='ieer', pattern=ROLES):\n",
        "      print (nltk.sem.relextract.clause(r, relsym=\"ROLES\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROLES('kivutha_kibwana', 'national_convention_assembly')\n",
            "ROLES('boban_boskovic', 'plastika')\n",
            "ROLES('robert_mergess', 'berkeley_center_for_law_and_technology')\n",
            "ROLES('jack_balkin', 'yale')\n",
            "ROLES('david_post', 'cyberspace_law_institute')\n",
            "ROLES('william_gale', 'brookings_institution')\n",
            "ROLES('joel_slemrod', 'university_of_michigan')\n",
            "ROLES('kaufman', 'tv_books_llc')\n",
            "ROLES('sherry_lansing', 'paramount_motion_picture_group')\n",
            "ROLES('rick_yorn', 'addis-wechsler_&_associates')\n",
            "ROLES('ken_kaess', 'ddb_needham')\n",
            "ROLES('norio_ohga', 'sony_corporation')\n",
            "ROLES('raymond_rosen', 'robert_wood_johnson_medical_school')\n",
            "ROLES('pepper_schwartz', 'university_of_washington')\n",
            "ROLES('irwin_goldstein', 'boston_university_school_of_medicine')\n",
            "ROLES('jennifer_berman', 'university_of_maryland')\n",
            "ROLES('anthony_chan', 'banc_one_investment_advisors_corp')\n",
            "ROLES('kevin_ashby', 'the_sun_advocate')\n",
            "ROLES('paul_volcker', 'us_federal_reserve')\n",
            "ROLES('israel_singer', 'world_jewish_congress')\n",
            "ROLES('katherine_abraham', 'bureau_of_labor_statistics')\n",
            "ROLES('lloyd_kiva_new', 'institute_of_american_indian_art')\n",
            "ROLES('barry_travis', 'little_rock_convention_and_visitors_bureau')\n",
            "ROLES('linda_ward', 'legacy_hotel')\n",
            "ROLES('j_jackson_walter', 'national_trust_for_historic_preservation')\n",
            "ROLES('dan_morgenstern', 'institute_of_jazz_studies')\n",
            "ROLES('peter_cannell', 'smithsonian_institution_press')\n",
            "ROLES('trimble', 'ulster_unionist_party')\n",
            "ROLES('seamus_mallon', 'social_democrats')\n",
            "ROLES('yegor_stroyev', 'upper_house')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLQ5IZkUUSIB"
      },
      "source": [
        "\n",
        "## 4. Inter-Annotator agreement ##\n",
        "\n",
        "As discussed in the lecture, Inter-Annotator Agreement (IAA) between two annotators can be calculated using Cohen's Kappa $(\\kappa)$ as follows:\n",
        "\n",
        "$$\\kappa = \\frac{P_o - P_e}{1-P_e}$$\n",
        "\n",
        "where $P_o$ is observed agreement and $P_e$ is expected agreement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FNvdqRhvYQf"
      },
      "source": [
        "### Exercise 4.1 ###\n",
        "\n",
        "For the given document:\n",
        "\n",
        "1. Annotate the given document with named entities using the IOB tagging scheme (annotate all of the named entities: PERson, LOCation, ORGanisation, TIME).\n",
        "2. Calculate the Inter-Annotator Agreement with one other student.\n",
        "3. Interpret the obtained value according to Landis and Koch scale.\n",
        "\n",
        "\n",
        "**Document 1**: *The fourth Wells account moving to another agency is the packaged paper-products division of \n",
        "Georgia-Pacific Corp., which arrived at Wells only last fall. Like Hertz and the History Channel, \n",
        "it is also leaving for an Omnicom-owned agency, the BBDO South unit of BBDO Worldwide. BBDO South in Atlanta, \n",
        "which handles corporate advertising for Georgia-Pacific, will assume additional duties for brands like Angel\n",
        "Soft toilet tissue and Sparkle paper towels, said Ken Haldin, a spokesman for Georgia-Pacific in Atlanta.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TZFfiHu1Xjx"
      },
      "source": [
        "Annotator 1:\n",
        "\n",
        "**O**: The **O**: fourth **O**: Wells **O**: account **O**: moving **O**: to **O**: another **O**: agency **O**: is **O**: the **O**: packaged **O**: paper-products **O**: division **O**: of **B_ORG**: Georgia-Pacific **I_ORG**: Corp. **O**: , **O**: which **O**: arrived **O**: at **O**: Wells **O**: only **O**: last **TIME**: fall **O**: . **O**: Like **O**: Hertz **O**: and **O**: the **O**: History **O**: Channel **O**: , **O**: it **O**: is **O**: also **O**: leaving **O**: for **O**: an **B_ORG**: Omnicom-owned **O**: agency **O**: , **O**: the **B_ORG**: BBDO **I_ORG**: South **O**: unit **O**: of **B_ORG**: BBDO **I_ORG**: Worldwide **O**: . **B_ORG**: BBDO **I_ORG**: South **O**: in **B_LOC**: Atlanta **O**: , **O**: which **O**: handles **O**: corporate **O**: advertising **O**: for **B_ORG**: Georgia-Pacific **O**: , **O**: will **O**: assume **O**: additional **O**: duties **O**: for **O**: brands **O**: like **B_ORG**: Angel **I_ORG**: Soft **O**: toilet **O**: tissue **O**: and **B_ORG**: Sparkle **O**: paper **O**: towels **O**: , **O**: said **B-PER**: Ken **I-PER**: Haldin **O**: , **O**: a **O**: spokesman **O**: for **B_ORG**: Georgia-Pacific **O**: in **B_LOC**: Atlanta **O**: .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmAJ8atx9h6r"
      },
      "source": [
        "Annotator 2:\n",
        "\n",
        "**O**: The **O**: fourth **B_ORG**: Wells **O**: account **O**: moving **O**: to **O**: another **O**: agency **O**: is **O**: the **O**: packaged **O**: paper-products **O**: division **O**: of **B_ORG**: Georgia-Pacific **I_ORG**: Corp. **O**: , **O**: which **O**: arrived **O**: at **B_ORG**: Wells **O**: only **O**: last **O**: fall **O**: . **O**: Like **B_ORG**: Hertz **O**: and **O**: the **B_ORG**: History **I_ORG**: Channel **O**: , **O**: it **O**: is **O**: also **O**: leaving **O**: for **O**: an **B_ORG**: Omnicom-owned **O**: agency **O**: , **O**: the **B_ORG**: BBDO **I_ORG**: South **I_ORG**: unit **O**: of **B_ORG**: BBDO **I_ORG**: Worldwide **O**: . **B_ORG**: BBDO **I_ORG**: South **O**: in **B_LOC**: Atlanta **O**: , **O**: which **O**: handles **O**: corporate **O**: advertising **O**: for **B_ORG**: Georgia-Pacific **O**: , **O**: will **O**: assume **O**: additional **O**: duties **O**: for **O**: brands **O**: like **B_ORG**: Angel **I_ORG**: Soft **O**: toilet **O**: tissue **O**: and **B_ORG**: Sparkle **O**: paper **O**: towels **O**: , **O**: said **B_PER**: Ken **I_PER**: Haldin **O**: , **O**: a **O**: spokesman **O**: for **B_ORG**: Georgia-Pacific **O**: in **B_LOC**: Atlanta **O**: ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXEZlYMx2bws"
      },
      "source": [
        "sentence = \"The fourth Wells account moving to another agency is the packaged paper-products division of \\\n",
        "Georgia-Pacific Corp., which arrived at Wells only last fall. Like Hertz and the History Channel, \\\n",
        "it is also leaving for an Omnicom-owned agency, the BBDO South unit of BBDO Worldwide. BBDO South in Atlanta, \\\n",
        "which handles corporate advertising for Georgia-Pacific, will assume additional duties for brands like Angel \\\n",
        "Soft toilet tissue and Sparkle paper towels, said Ken Haldin, a spokesman for Georgia-Pacific in Atlanta.\"\n",
        "\n",
        "tokens = nltk.word_tokenize(sentence)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg3fwI2H53nI",
        "outputId": "9267452d-4420-4992-ba5d-f51219858476"
      },
      "source": [
        "# Download annotations.\n",
        "!curl https://pastebin.com/raw/Ka19SRJp > annotation-1.tsv   # Omnia's annotation\n",
        "!curl https://pastebin.com/raw/yZ5Ker55 > annotation-2.tsv   # Nivranshu's annotation"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   863    0   863    0     0  10034      0 --:--:-- --:--:-- --:--:-- 10034\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   884    0   884    0     0  10045      0 --:--:-- --:--:-- --:--:-- 10045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTrcYSU13s7s"
      },
      "source": [
        "# Load annotations.\n",
        "with open('annotation-1.tsv') as f:\n",
        "  annotation_1 = [line.strip().split('\\t')[-1] for line in f.readlines()]\n",
        "with open('annotation-2.tsv') as f:\n",
        "  annotation_2 = [line.strip().split('\\t')[-1] for line in f.readlines()]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIQpXP-z8VgH",
        "outputId": "2b0a39a5-53c7-4f1c-c896-31b5a05e0f1e"
      },
      "source": [
        "# Calculate Cohen's Kappa.\n",
        "\n",
        "# Using sklearn's implementation. See details on the link below.\n",
        "# (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html)\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "cohen_kappa_score(annotation_1, annotation_2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7503152585119799"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV3QfW316eY8",
        "outputId": "88f5e7d9-bf85-4b83-f0b5-26d2b60d1262"
      },
      "source": [
        "# nltk also has a function to calculate Cohen's Kappa but it requires \n",
        "# the annotations to be structured as a list containing (coder, item, label).\n",
        "# More information here: https://www.nltk.org/_modules/nltk/metrics/agreement.html\n",
        "\n",
        "from nltk.metrics.agreement import AnnotationTask\n",
        "\n",
        "# create a single list of annotations in the format (coder, item, label).\n",
        "annotations = [(1, idx, label) for (idx, label) in enumerate(annotation_1)] + \\\n",
        "              [(2, idx, label) for (idx, label) in enumerate(annotation_2)]\n",
        "\n",
        "task = AnnotationTask(data=annotations)\n",
        "task.kappa()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7503152585119798"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6wdBXIFSai9"
      },
      "source": [
        "### Exercise 4.2\n",
        "\n",
        "As discussed in the lecture, to improve the obtained Kappa value we can:\n",
        "- adjust annotation guidelines for re-annotation\n",
        "- discuss disagreements between annotators\n",
        "\n",
        "\n",
        "In this exercise, design a set of annotation guidelines with other student(s) and annotate the document below as per those guidelines. Also, calculate IAA after annotation.\n",
        "\n",
        "**Document 2**: *The maker of farm equipment said the three-year labor agreement with the International Association of Machinists and Aerospace Workers at John Deere Horicon Works, Deere's primary facility for producing lawn and grounds-care equipment, takes effect immediately and extends through Oct. 1 , 1992.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM-Vmqzv1egj"
      },
      "source": [
        "# Left for the students"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOBXzAbFqA8K"
      },
      "source": [
        "---\n",
        "#### Resources for Regular Expressions:\n",
        "\n",
        "1. [RegexOne - Learn Regular Expressions - Lesson 1: An Introduction, and the ABCs](https://regexone.com/)\n",
        "1. [Online regex tester and debugger: PHP, PCRE, Python, Golang and JavaScript](https://regex101.com/)\n",
        "1. [re — Regular expression operations — Python 3.9.0 documentation](https://docs.python.org/3/library/re.html)"
      ]
    }
  ]
}