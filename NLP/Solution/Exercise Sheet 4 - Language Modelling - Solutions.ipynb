{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exercise Sheet 4 - Language Modelling - Solutions.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hIZp5iLCJsNf"},"source":["# Exercise Sheet 4 - Language Modelling - Solutions"]},{"cell_type":"markdown","metadata":{"id":"Mcp-LZu8JrTv"},"source":["## Learning Objectives\n","\n","In this lab we are going to:\n","\n","- Play around with text corpora <br>\n","- Learn some statistics tricks in Python and NLTK <br>\n","- Learn about language modelling <br>\n","- Learn about n-grams <br>\n","- Naive bayes as a lanugage model <br>\n","- Hands-on data sparsity and smoothing techniques <br>\n"]},{"cell_type":"code","metadata":{"id":"mOvc51Tj2rcG","executionInfo":{"status":"ok","timestamp":1603441762279,"user_tz":-60,"elapsed":2435,"user":{"displayName":"John McCrae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSNrSveUKuY4mbk4beGM7HV_qygHx4WRNqxnOwhw=s64","userId":"00135099661196735771"}},"outputId":"3e7e2d06-0044-43ae-c80a-69d3dbfc37f0","colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# setting the stage ;)\n","import nltk\n","nltk.download('brown')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"h1hyTgjNJrTx"},"source":["--------------------\n","## Text Corpus: Statistics and Probability\n","\n","### Accessing the corpus\n","Open a Python session and  obtain the <a href=\"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/brown.zip\">Brown corpus</a>, using NLTK."]},{"cell_type":"markdown","metadata":{"id":"IPZGc_taJsNx"},"source":["**Exercise 1:**\n","\n","What is the frequency of the word (ignoring case) &lsquo;world&rsquo; in the news category in Brown corpus?"]},{"cell_type":"code","metadata":{"id":"Kk739nKkbLUa"},"source":["# You will need to import 'Brown' as follows:\n","from nltk.corpus import brown\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6SWAJ0nWJsNy","executionInfo":{"status":"ok","timestamp":1603441771403,"user_tz":-60,"elapsed":1326,"user":{"displayName":"John McCrae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSNrSveUKuY4mbk4beGM7HV_qygHx4WRNqxnOwhw=s64","userId":"00135099661196735771"}},"outputId":"af0828ab-a687-456e-9ba2-73f6a9119b17","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["def count_freq(category, given_word):\n","    count = 0\n","    for word in brown.words(categories=category):\n","        word = word.lower()\n","        if word == given_word:\n","            count += 1\n","    return count\n","\n","print(count_freq('news', 'world'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["46\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vhJYR0pJJrUL"},"source":["### Frequency of Words\n","\n","We can easily get the frequency distribution of the words in a corpus as follows:"]},{"cell_type":"markdown","metadata":{"id":"j68-epFbJsN9"},"source":["**Exercise 2:**\n","In the Brown Corpus, in which category(s) of the  news, government and editorial categories, the word (ignoring case) &lsquo;world&rsquo; has the highest total frequency?\n","* news\n","* government\n","* editorial"]},{"cell_type":"code","metadata":{"id":"u7xQiOybJsN-","executionInfo":{"status":"ok","timestamp":1603441785621,"user_tz":-60,"elapsed":1380,"user":{"displayName":"John McCrae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSNrSveUKuY4mbk4beGM7HV_qygHx4WRNqxnOwhw=s64","userId":"00135099661196735771"}},"outputId":"53a055aa-c359-4b4c-bd67-a4323e8d2eb3","colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["print(count_freq('news', 'world'))\n","print(count_freq('government', 'world'))\n","print(count_freq('editorial', 'world'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["46\n","51\n","77\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0E0wMa_lJsOC"},"source":["### Probabilities\n","\n","**Exercise 3:**\n","Calculate probabilities (relative frequency) of all words for only __news__ category in Brown corpora.\n","What is the probability of the words &lsquo;jury&rsquo; and &lsquo;government&rsquo;?"]},{"cell_type":"code","metadata":{"id":"2fIajkqTJsOD","executionInfo":{"status":"ok","timestamp":1603441793314,"user_tz":-60,"elapsed":1044,"user":{"displayName":"John McCrae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSNrSveUKuY4mbk4beGM7HV_qygHx4WRNqxnOwhw=s64","userId":"00135099661196735771"}},"outputId":"9eb9418c-8e1c-4806-d277-79cd9578f2fa","colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["def prob(category, given_word):\n","    count = 0\n","    total = 0\n","\n","    for word in brown.words(categories=category):\n","        word = word.lower()\n","        if word == given_word:\n","            count += 1\n","        total += 1\n","            \n","    return float(count)/total\n","\n","print(prob('news','jury'))\n","print(prob('news','government'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.00045746564035244744\n","0.000725978081428884\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8b0sVdz1JrUQ"},"source":["--------------------\n","## N-Grams\n","\n","The probabilisic Language Models (a.k.a n-gram LMs) are developed to construct the joint probability distribution of a sequence of words. Based on the Markov assumption, the process of predicting a word sequence is broken up into predicting one word at a time.\n","\n","We can extract unigrams, and bigrams from a corpus as follows:\n","In this example, we are going to generate unigrams and bigrams from the novel *Emma* by Jane Austen from The Gutenberg Corpus"]},{"cell_type":"markdown","metadata":{"id":"Ke4_uXkvJsOe"},"source":["**Exercise 4:**\n","Write a function to find the most common phrases (trigrams) in the __fiction__ category of the brown corpus."]},{"cell_type":"code","metadata":{"id":"DaNKUaVxJsOe","executionInfo":{"status":"ok","timestamp":1603441807141,"user_tz":-60,"elapsed":751,"user":{"displayName":"John McCrae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSNrSveUKuY4mbk4beGM7HV_qygHx4WRNqxnOwhw=s64","userId":"00135099661196735771"}},"outputId":"3c18ebef-da6b-4f31-d847-1776732b1278","colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["from nltk.corpus import brown\n","\n","fiction_text = brown.words(categories='fiction')\n","trigram =  [t for t in nltk.trigrams(fiction_text)]\n","freq = nltk.FreqDist(trigram) #have you noticed the difference between ConditionalFreqDist and FreqDist!\n","freq.most_common(20)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[((\"''\", '?', '?'), 128),\n"," ((\"''\", '.', '``'), 117),\n"," (('.', 'He', 'was'), 76),\n"," (('.', 'It', 'was'), 73),\n"," ((\"''\", '!', '!'), 64),\n"," (('.', 'He', 'had'), 53),\n"," (('.', '``', 'I'), 53),\n"," (('?', '?', '``'), 45),\n"," ((',', 'and', 'the'), 42),\n"," (('.', 'There', 'was'), 40),\n"," ((\"''\", ',', 'he'), 37),\n"," (('said', '.', '``'), 33),\n"," ((',', 'and', 'he'), 33),\n"," (('said', ',', '``'), 30),\n"," (('.', 'She', 'was'), 30),\n"," ((\"''\", ',', 'she'), 29),\n"," ((\"''\", '.', 'He'), 28),\n"," ((',', 'he', 'said'), 26),\n"," (('one', 'of', 'the'), 24),\n"," (('.', 'In', 'the'), 24)]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"6rWK4t9GJsOi"},"source":["--------------\n","## Probabilistic modeling\n","\n","## NaÃ¯ve Bayes\tas\ta\tLanguage\tModel\n","Based on probabilities of words in only the news and fiction categories in the brown corpus, classify the phrase 'mysterious murder case' to one of these categories. \n","\n","You should implement Naive Bayes classifier using probabilities of each word:\n","\n","$P(fiction|mysterious\\ murder\\ case) \\propto P(mysterious|fiction) \\times P(murder|fiction) \\times P(case|fiction) \\times P(fiction)$\n","where $P(news) = 0.5$ and $P(fiction) = 0.5$\n","\n","**Exercise 5:**\n","Write a general purpose Naive Bayes classifier such as follows:"]},{"cell_type":"code","metadata":{"id":"RU4kDNJXJsOk","executionInfo":{"status":"ok","timestamp":1603441815726,"user_tz":-60,"elapsed":947,"user":{"displayName":"John McCrae","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhSNrSveUKuY4mbk4beGM7HV_qygHx4WRNqxnOwhw=s64","userId":"00135099661196735771"}},"outputId":"b27dd2cb-9c18-49e2-da87-cbf337b18266","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from random import random\n","\n","def calculate_probability(phrase, category):\n","    p = 1.0\n","    for word in phrase.split():\n","        word = word.lower()\n","        p *= prob(category, word)\n","        return p * 0.5\n","\n","def naive_bayes(phrase):\n","    news_prob = calculate_probability(phrase, 'news')\n","    fiction_prob = calculate_probability(phrase, 'fiction')\n","    if news_prob > fiction_prob:\n","        return 0 #news\n","    else:\n","        return 1 #fiction\n","\n","print(naive_bayes(\"mysterious murder case\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xq9JLzlOJsOo"},"source":["### Smoothing\n","\n","A simple n-gram model would give zero probability to all of the combination that were not encountered in the training corpus, i.e. it would most likely give zero probability to most of the out-of-sample test cases. This problem is known as data sparsity and the traditional solution to it is to use smoothing techniques."]},{"cell_type":"markdown","metadata":{"id":"HkmHcqj2JsOo"},"source":["#### Example: bigram model\n","\n","(pen and paper exercises)\n","\n","Given Corpus:\n","\n","$JOHN\\ READ\\ MOBY\\ DICK$\n","<br>\n","$MARY\\ READ\\ A\\ DIFFERENT\\ BOOK$\n","<br>\n","$SHE\\ READ\\ A\\ BOOK\\ BY\\ CHER$\n"]},{"cell_type":"markdown","metadata":{"id":"DRUn85AHJsOo"},"source":["**Exercise 6:**\n","Calculate the probability of the sentence \"JOHN READ A BOOK\"?\n","\n","p(john read a book)\n","\n","= p(john) Ã p(read|john) Ã p(a|read) Ã p(book|a) \n","\n","= 1/15 Ã 1/1 Ã 2/3 Ã 1/2\n","\n","= 0.022222222\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_k_g-EAyJsOp"},"source":["**Exercise 7:**\n","What is the $p(CHER\\ READ\\ A\\ BOOK)$?\n","\n","p(cher read a book)\n","\n","= p(cher) Ã p(read|cher) Ã p(a|read) Ã p(book|a) \n","\n","= 1/15 Ã 0/0 Ã 2/3 Ã 1/2\n","\n","= 0"]},{"cell_type":"markdown","metadata":{"id":"MnrWbm90kHTk"},"source":["$p(w_i|w_{i-1}) = \\frac{1 + c(w_{iâ1} w_i)} {\\sum_{w'_i}{c( w'_i) }+ |V|}$"]},{"cell_type":"markdown","metadata":{"id":"te21TeaNJsOq"},"source":["### Add-one smoothing\n","\n","$p(w_i|w_{i-1}) = \\frac{1 + c(w_{iâ1} w_i)} {\\sum_{w'_i} [1 + c(w_{iâ1} w'_i)] }$"]},{"cell_type":"markdown","metadata":{"id":"-lzNDOr2JsOq"},"source":["**Exercise 8:**\n","Re-calculate the $p(JOHN\\ READ\\ A\\ BOOK)$ and $p(CHER\\ READ\\ A\\ BOOK)$ using add-one smoothing\n","\n","p(john read a book)\n","\n","= p(john) Ã p(read|john) Ã p(a|read) Ã p(book|a) \n","\n","= (1+1)/(11+15) Ã (1+1)/(11+1) Ã (1+2)/(11+3) Ã (1+1)/(11+2)\n","\n","= 0.000422654\n","\n","\n","\n","\n","p(cher read a book)\n","\n","= p(cher) Ã p(read|cher) Ã p(a|read) Ã p(book|a) \n","\n","= (1+1)/(11+15) Ã (1+0)/(11+0) Ã (1+2)/(11+3) Ã (1+1)/(11+2)\n","\n","= 0.000230539\n","\n","\n"]}]}