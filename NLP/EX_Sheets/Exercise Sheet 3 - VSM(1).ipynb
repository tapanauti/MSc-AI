{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Exercise Sheet 3 - VSM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M_v-BtFh0dr"
      },
      "source": [
        "# Exercise Sheet 3 - Vector Space Models and Word Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_op0C-mNsyIN"
      },
      "source": [
        "## Learning Objectives\n",
        "    - Introduction to word vectors\n",
        "    - Overview on distributional semantics\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26KdXQvBtyrG"
      },
      "source": [
        "# setting the stage ;)\n",
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JMJATpKsyIO"
      },
      "source": [
        "# Preprocessing: Tokenization & POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXvxlx6dsyIO"
      },
      "source": [
        "# Sentence tokenization with nltk\n",
        "import nltk\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UpaRe6KsyIU"
      },
      "source": [
        "# Part-of-Speech (POS) Tagging\n",
        "\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "print(tagged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0fDIB2xsyIl"
      },
      "source": [
        "### Exercise 1: How to get rid of the punctuations from the text?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaN6BGKyXAjt"
      },
      "source": [
        "text = \"Remember, remember, the fifth of November, Gunpowder, treason and plot! If you can't give us one, we'll take two; The better for us and the worse for you!\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "tokens = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Your code goes here\n",
        "\n",
        "# p.s.: print your_refined_tokens_without_punct\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGL290idYD3_"
      },
      "source": [
        "Hint: a list of punctuations could be used (Check string.punctuation on https://docs.python.org/3/library/string.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLWR4i0QsyIu"
      },
      "source": [
        "----\n",
        "# Distributional Semantics & Word Vectors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f8vXuCWgRIC"
      },
      "source": [
        "## Context Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sICAeOzNgeOn"
      },
      "source": [
        "### Exercise 2: looks up every occurrence of the word \"*affection*\" and prints out it's context in  the text of Sense and Sensibility by Jane Austen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d2RH8MKgvCs"
      },
      "source": [
        "# your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_xZItbMhe3X"
      },
      "source": [
        "Hint: We have already done that in Exercise Sheet 1.1 using NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3DHpiA6gOJ6"
      },
      "source": [
        "## n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Na5OI8LsyJC"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "text = \"You shall know a word by the company it keeps meaning - Firth (1957)\"\n",
        "tokenize = nltk.word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-t50lnDsyJF"
      },
      "source": [
        "# show all possible n-grams\n",
        "bigrams = ngrams(tokenize,2)\n",
        "for b in bigrams:\n",
        "    print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDoXkyFQsyJI"
      },
      "source": [
        "### Frequency of occurence for each bigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axcfICx5syJK"
      },
      "source": [
        "#### showing n-grams with raw frequency\n",
        "\n",
        "from nltk.collocations import *\n",
        "import nltk\n",
        "#You should tokenize your text\n",
        "text = \"The cat lies on the mat and a dog lies on the floor\"\n",
        "tokens = nltk.wordpunct_tokenize(text)\n",
        "bigrams = BigramCollocationFinder.from_words(tokens)\n",
        "for bigram, freq in bigrams.ngram_fd.items():  \n",
        "      print(bigram, freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9FMK0x3VKN5"
      },
      "source": [
        "from nltk.book import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G4v5hoJsyJP"
      },
      "source": [
        "# show n-grams measured using Pointwise Mutual Information\n",
        "\n",
        "from nltk.collocations import *\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "\n",
        "#finder = BigramCollocationFinder.from_words(nltk.corpus.genesis.words('english-web.txt'))\n",
        "finder = BigramCollocationFinder.from_words(text1)\n",
        "\n",
        "finder.nbest(bigram_measures.pmi, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ57fYl_syJR"
      },
      "source": [
        "... making PMI more interpretable, print out only n-grams which apear about a certain threshold "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L3mlQCGsyJR"
      },
      "source": [
        "freq_threshold = 30\n",
        "finder = BigramCollocationFinder.from_words(text1)\n",
        "finder.apply_freq_filter(freq_threshold)\n",
        "finder.nbest(bigram_measures.pmi, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MicxTCPasyJU"
      },
      "source": [
        "### Exercise 3: Interpret the variation in pmi based on frequency threshold?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znicrPcHf6Md"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srf3NULh3AWh"
      },
      "source": [
        "Hint: Change frequency threshold to 100 and 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGsBiblqWpdG"
      },
      "source": [
        "*Please write your interpretation here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz0TZdMTcsH-"
      },
      "source": [
        "## Pointwise Mutual Information (PMI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRrh1llsex_1"
      },
      "source": [
        "import nltk\n",
        "from nltk.collocations import *\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"eat sleep code repeat sleep dream code repeat\"\n",
        "\n",
        "Bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(word_tokenize(text))\n",
        "\n",
        "for i in finder.score_ngrams(Bigram_measures.pmi):\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaOnODVifDkG"
      },
      "source": [
        "### Exercise 4: What role PMI may play in language modelling?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT_VkuAfgRUx"
      },
      "source": [
        "*Please write your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOdBaoatsyJZ"
      },
      "source": [
        "\n",
        "# Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "L__0nWIZsyJZ"
      },
      "source": [
        "# example of a sparse word vector\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        " \n",
        "corpus = [\n",
        "'All my cats in a row.',\n",
        "'When my cat sits down, she looks like a Furby toy!',\n",
        "'The cat from outer space',\n",
        "'Sunshine loves to sit like this for some reason.'\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "print(vectorizer.fit_transform(corpus).todense())\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WrLXrZb4pr0"
      },
      "source": [
        "### Exercise 5: Write your intepretation of above vector representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJm03kGF9Jop"
      },
      "source": [
        "*Please write your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzmlV3WVsyJb"
      },
      "source": [
        "## Word Co-occurrence Matrix in sparse CRS (Compressed Sparse Row) format\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBdNRjtDsyJb"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "docs = ['the cat lies on the map',\n",
        "        'the cat lies on the floor',\n",
        "        'an cat sits near the floor']\n",
        "\n",
        "# docs = [\n",
        "# 'All my cats in a row.',\n",
        "# 'When my cat sits down, she looks like a Furby toy!',\n",
        "# 'The cat from outer space',\n",
        "# 'Sunshine loves to sit like this for some reason.'\n",
        "# ]\n",
        "\n",
        "count_model = CountVectorizer(ngram_range=(1,1)) # default unigram model\n",
        "X = count_model.fit_transform(docs)\n",
        "Xc = (X.T * X)\n",
        "Xc.setdiag(0)\n",
        "print(count_model.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uOLOMbLsyJd"
      },
      "source": [
        "print(Xc.toarray()) # print out matrix in dense format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG3kSJqCsyJf"
      },
      "source": [
        "### Exercise 6: Explain the matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy4JB5y1ifNT"
      },
      "source": [
        "*Write your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwP5JerFsyJl"
      },
      "source": [
        "# Word2Vec\n",
        "\n",
        "Mikolov, Tomas; et al. \"Efficient Estimation of Word Representations in Vector Space\". arXiv:1301.3781\n",
        "\n",
        "Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.[Wikipedia]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC77cEZwsyJo"
      },
      "source": [
        "# gensim - open-source vector space modeling and topic modeling toolkit\n",
        "\n",
        "import gensim\n",
        "from nltk.corpus import brown\n",
        "model = gensim.models.Word2Vec(brown.sents())\n",
        "model.save('brown.embedding')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCn-mlETsyJq"
      },
      "source": [
        "new_model = gensim.models.Word2Vec.load('brown.embedding')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqa-VdVFsyJu"
      },
      "source": [
        "# word vector dimensionality\n",
        "\n",
        "len(new_model['university'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k66-_Z3VsyJx"
      },
      "source": [
        "# show word similarity between words, calculated on the non-zero word vectors\n",
        "\n",
        "new_model.similarity('university','school')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-caXf8dsyJ0"
      },
      "source": [
        "### Exercise 7: Calculate and interpret the similarity between (bank, river) and (bank, deposit)? How will you handle word sense disambiguation?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bFxvlPBsyJ1"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsBoLdrspReJ"
      },
      "source": [
        "*Your answer goes here*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp3mhluasLgO"
      },
      "source": [
        "# # download the Google newsW1v model from \n",
        "! wget \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fydqymFy5aen"
      },
      "source": [
        "# Mount your drive to the colab notebook\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\") #authorization is required here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI0xARyk17jN"
      },
      "source": [
        "from nltk.data import find\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "# Mention path where GoogleNews-vectors-negative300.bin.gz file is downloaded (it should be on the same loacation where your colab notebook is)\n",
        "filename = '/content/drive/My Drive/Colab Notebooks/GoogleNews-vectors-negative300.bin.gz'\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtL1KKXI163E"
      },
      "source": [
        "# number of entries in the Word2Vec matrix\n",
        "\n",
        "len(model.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adh9rqHl16Hs"
      },
      "source": [
        "# dimensionality of of the dense word vectors\n",
        "\n",
        "len(model['university'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXPkB68psyKB"
      },
      "source": [
        "# most similar words based on Word2Vec\n",
        "\n",
        "model.most_similar(positive=['soccer'], topn = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybTqgcpnsyKH"
      },
      "source": [
        "# most dissimilar entry among provided words\n",
        "\n",
        "model.doesnt_match('wrestling cooking dinner potato'.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkIPGmXlsyKL"
      },
      "source": [
        "## Vector operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjThcGUPsyKM"
      },
      "source": [
        "model.most_similar(positive=['woman','king'], negative=['man'], topn = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gT7SN3jrsyKO"
      },
      "source": [
        "model.most_similar(positive=['Delhi','Russia'], negative=['Mosco'], topn = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXzCXSfYsyKP"
      },
      "source": [
        "### Excercise 8: Can you think of similar examples?\n",
        "\n",
        "Actor - male + female?\n",
        "\n",
        "batman - bat + spider?\n",
        "\n",
        "summer - sun + cold?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCUGMLUpNewy"
      },
      "source": [
        "*Guess the result here before executing the code*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLKf_Yh_syKR"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNAd3mz_dnkZ"
      },
      "source": [
        "# Visualizing Vector Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1OBxo1Cdm5-"
      },
      "source": [
        "import numpy as np\n",
        "labels = []\n",
        "count = 0\n",
        "max_count = 50\n",
        "X = np.zeros(shape=(max_count,len(model['university'])))\n",
        "\n",
        "for term in model.vocab:\n",
        "    X[count] = model[term]\n",
        "    labels.append(term)\n",
        "    count+= 1\n",
        "    if count >= max_count: break\n",
        "\n",
        "# It is recommended to use PCA first to reduce to ~50 dimensions\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=50)\n",
        "X_50 = pca.fit_transform(X)\n",
        "\n",
        "# Using TSNE to further reduce to 2 dimensions\n",
        "from sklearn.manifold import TSNE\n",
        "model_tsne = TSNE(n_components=2, random_state=0)\n",
        "Y = model_tsne.fit_transform(X_50)\n",
        "\n",
        "# Show the scatter plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(Y[:,0], Y[:,1], 20)\n",
        "\n",
        "# Add labels\n",
        "for label, x, y in zip(labels, Y[:, 0], Y[:, 1]):\n",
        "    plt.annotate(label, xy = (x,y), xytext = (0, 0), textcoords = 'offset points', size = 10)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}