{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise Sheet 5 - POS Tagging .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-HXJjMYiWmd"
      },
      "source": [
        "# Exercise Sheet 5 - POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycUr3uVq6HRI"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "In this lab we are going to:\n",
        "\n",
        "- Explore POS Tagging using NLTK <br>\n",
        "- Hidden Markov Models (HMM) <br>\n",
        "- Learn POS tagging with HMM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRYvTYQI6HRJ"
      },
      "source": [
        "----------------\n",
        "## POS Tagging \n",
        "\n",
        "### Approaches\n",
        "\n",
        "In POS tagging, we have a sentence X, and want to predict the part of speech of each word in the sentence Y. This can be done in different ways:\n",
        " \n",
        "1- Pointwise prediction: a classifier that predicts each word individually such as perceptron. <br>\n",
        "2- Generative sequence models: a probabilistic model that assigns probabilities to sequence of words such as Hidden Markov Model.** [the focus of this lab]** <br>\n",
        "3- Discriminative sequence models: predict whole sequence with a classifier such as conditional random fields (CRF). <br>\n",
        "\n",
        "### Tags Set\n",
        "The most common tags sets are:\n",
        "\n",
        "1- <a href= \"http://ucrel.lancs.ac.uk/claws5tags.html\">Claws5</a>: 62 different tags <br>\n",
        "2- <a href=\"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">Penn Treebank</a>: 45 different tags (Most widely used currently) <br>\n",
        "3- <a href = \"http://www.comp.leeds.ac.uk/ccalas/tagsets/brown.html\">The Brown Corpus tagset</a>: (87 tags)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_zZOmHY6HRK"
      },
      "source": [
        "\n",
        "### NLTK POS Tagging\n",
        "\n",
        "The NLTK tagger can be used as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_qQ7aQEULv5"
      },
      "source": [
        "#setting the stage ;)\n",
        "# if you encounter some errors related to missing nltk packages run the following commands\n",
        "\n",
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6ilQWhx6HRL"
      },
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = word_tokenize(\"And now for something completely different\")\n",
        "nltk.pos_tag(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewPLRHIu6HRO"
      },
      "source": [
        "The brown corpus has been manually tagged with part-of-speech tags which is useful for testing taggers and for training statistical taggers. In order to read a tagged corpus we can use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcBdV6f46HRP"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "\n",
        "print (brown.tagged_words())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXqMWjxi6HRS"
      },
      "source": [
        "**Exercise 1:**\n",
        "Count each POS tag assigned to the word **(ignore case)** \"world\" in the **news** category of the brown corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IhUjs9cVXyt"
      },
      "source": [
        "#your code goes here; output should be: NN: 37, NN-TL: 9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEFQen596HRT"
      },
      "source": [
        "**Exercise 2:**\n",
        "can you get the frequency distribution of each tag in the brown corpus?  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyBqwho6VzhC"
      },
      "source": [
        "#your code goes here; output should be \n",
        "#[('NN', 152470),('IN', 120557),('AT', 97959),....]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irYKYRLeiWm2"
      },
      "source": [
        "**Exercise 3:**\n",
        "What are the most common verbs in **fiction** category in the brown corpus? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9OYyFT2iWm3"
      },
      "source": [
        "#your code goes here; output should be \n",
        "#['came': 'VBD', 'curled': 'VBD', 'ki-yi-ing': 'VBG',....]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e75mJQ26HRU"
      },
      "source": [
        "-----------------------\n",
        "## Hidden Markov Model\n",
        "\n",
        "The sequence of tags can be veiwed as a Markov chain so let us explore the construction and solution of a Hidden Markov Model. Consider that we have an HMM with hidden states Noun, Verb, Adj and the following transition probability where $p(Y_{i+1}|Y_i)$ is the probability of state $Y_{i+1}$ occuring after $Y_i$ and the table of probabilities is as follows:\n",
        "\n",
        "| $p(Y_{i+1}|Y_i)$ | $Y_{i+1}$=Noun | $Y_{i+1}$=Verb | $Y_{i+1}$=Adj |\n",
        "|:-----------------|:--------------:|:--------------:|:-------------:|\n",
        "| $Y_i$=Start      |  0.5           |  0.4           | 0.1           |\n",
        "| $Y_i$=Noun       |  0.3           |  0.5           | 0.2           |\n",
        "| $Y_i$=Verb       |  0.7           |  0.2           | 0.1           |\n",
        "| $Y_i$=Adj        |  0.8           |  0.1           | 0.1           |\n",
        "\n",
        "Furthermore, consider that the model has a vocabulary as follows, with the probability of $p(X_i|Y_i)$ as follows \n",
        "\n",
        "| $p(X_i|Y_i)$ | cats | dogs | drink | water | milk | fresh |\n",
        "|:-------------|:----:|:----:|:-----:|:-----:|:----:|:-----:|\n",
        "| $Y_i$=Noun   | 0.2  | 0.2  |  0.2  | 0.2   | 0.1  | 0.0   |\n",
        "| $Y_i$=Verb   | 0.1  | 0.1  | 0.4   | 0.2   | 0.1  | 0.1   |\n",
        "| $Y_i$=Adj    | 0.0  | 0.0  | 0.2   | 0.0   | 0.2  | 0.8   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtxiCCz-6HRV"
      },
      "source": [
        "**Exercise 4:**\n",
        "\n",
        "Implement the above table and write a function that takes a sequence of words and a sequence of part-of-speech tags and returns the probability using the above model. Calculate the probability of the sentence \"cats drink fresh milk\" given the tags \"noun verb adj noun\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjWdJu-36HRW"
      },
      "source": [
        "tags = [\"start\",\"noun\",\"verb\",\"adj\"]\n",
        "words = [\"cats\",\"dogs\",\"drink\",\"water\",\"milk\",\"fresh\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5LuZ9y06HRY"
      },
      "source": [
        "def hmm_prob_with_state(words, tags):\n",
        "    prob = 1.0\n",
        "    # TODO\n",
        "    return prob\n",
        "\n",
        "print(hmm_prob_with_state([\"cats\",\"drink\",\"fresh\",\"milk\"],\n",
        "                          [\"noun\",\"verb\",\"adj\",\"noun\"]))\n",
        "#expected output should be 0.000128\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyF63Mt76HRc"
      },
      "source": [
        "**Exercise 5:**\n",
        "\n",
        "Using the Forward (dynamic programming) algorithm, write a function that calculates the likelihood of a sequence of words. Find the probability of the sentence \"Cats drink fresh milk\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfp_tl3m6HRd"
      },
      "source": [
        "def hmm_lm(words):\n",
        "    prob = 1.0\n",
        "    # TODO\n",
        "    return prob\n",
        "\n",
        "print(hmm_lm([\"cats\",\"drink\",\"fresh\",\"milk\"]))\n",
        "#expected output should be 0.00057068"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxY-BPjN6HRg"
      },
      "source": [
        "**Exercise 6:**\n",
        "\n",
        "Write a function that finds the most likely sequence of part-of-speech tags for a given sequence of words using the Viterbi algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hcz9PXFi6HRg"
      },
      "source": [
        "def hmm_map(words):\n",
        "    seq = [\"noun\",\"noun\",\"noun\",\"noun\"]\n",
        "    # TODO\n",
        "    return seq\n",
        "\n",
        "print(hmm_map([\"cats\",\"drink\",\"fresh\",\"milk\"]))\n",
        "#expected ouptut should be ['noun', 'verb', 'adj', 'noun']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hoxYGEA6HRj"
      },
      "source": [
        "**Exercise 7:**\n",
        "\n",
        "Consider the following corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQgecDP_iviU"
      },
      "source": [
        "sentences = [\n",
        "    [\"cats\",\"drink\",\"milk\"],\n",
        "    [\"dogs\",\"drink\",\"water\"],\n",
        "    [\"fresh\",\"milk\"],\n",
        "    [\"dogs\",\"drink\",\"fresh\",\"milk\"],\n",
        "    [\"cats\",\"milk\"]\n",
        "]\n",
        "\n",
        "tagged = [\n",
        "    [\"noun\",\"verb\",\"noun\"],\n",
        "    [\"noun\",\"verb\",\"noun\"],\n",
        "    [\"adj\",\"noun\"],\n",
        "    [\"noun\",\"verb\",\"adj\",\"noun\"],\n",
        "    [\"noun\",\"noun\"]\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POEj0zpO6HRm"
      },
      "source": [
        "Write a function that learns the emission and transition probabilities for the Hidden Markov Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU4rcM7L6HRn"
      },
      "source": [
        "def hmm_learn(sentences, tagged):\n",
        "    transitions = {t:{t2:0.0 for t2 in tags} for t in tags}\n",
        "    emissions    = {t:{w:0.0 for w in words} for t in tags}\n",
        "    # TODO\n",
        "    return transitions, emissions\n",
        "\n",
        "print(hmm_learn(sentences, tagged))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAqqsXeI6HRp"
      },
      "source": [
        "**Exercise 8:**\n",
        "\n",
        "Using the probability matrices you calculated in exercise 7, show that the probability of the sentence \"fresh fresh milk\" is zero. Suggest how you could change your calculation in exercise 7 to ensure that no sentence produces zero probability?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-MS1noO6HRq"
      },
      "source": [
        "transitions, emissions = hmm_learn(sentences, tagged)\n",
        "\n",
        "print(hmm_lm([\"fresh\",\"fresh\",\"milk\"]))\n",
        "\n",
        "def hmm_learn2(sentences, tagged):\n",
        "    transitions = {t:{t2:0.0 for t2 in tags} for t in tags}\n",
        "    emissions    = {t:{w:0.0 for w in words} for t in tags}\n",
        "    # TODO\n",
        "    return transitions, emissions\n",
        "\n",
        "transitions, emissions = hmm_learn2(sentences, tagged)\n",
        "\n",
        "print(hmm_lm([\"fresh\",\"fresh\",\"milk\"]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}