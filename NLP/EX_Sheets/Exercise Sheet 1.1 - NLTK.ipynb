{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Exercise Sheet 1.1 - NLTK.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VjXtQIEBqe3i"},"source":["# Exercise Sheet 1.1 - Text Processing with Python and NLTK\n"]},{"cell_type":"markdown","metadata":{"id":"RA4X4gQDsFRR"},"source":["## Learning Objectives\n","\n","The motivation of this exercise is to incorporate what you have learned in **Exercise Sheet 1.0** on text processing using python programming with the NLTK library. The NLTK is short for the Natural Language Toolkit and it \"*is a suite of open source Python modules, data sets, and tutorials supporting research and development in NLP.*\""]},{"cell_type":"markdown","metadata":{"id":"puwQW_jW4cF4"},"source":["---\n","### References\n","#### This notebook was adapted from Chapter 1 of the book [Natural Language Processing with Python](http://www.nltk.org/book/)."]},{"cell_type":"markdown","metadata":{"id":"GJ6xaZCqjU-z"},"source":["----\n","# 1. Getting Started With NLTK\n","If you are working with this notebook on Google Colab or already have `nltk` installed on your machine, you should be able to run the following code cell without any errors. \n","\n","Otherwise, you may seen an an `ImportError`, which means you need to install the `nltk` python package. From a command prompt, this will usually look something like \"`pip install nltk`\" or \"`conda install nltk`\" if you're using anaconda.\n","\n","You'll see `In [*]` to the left of the cell while nltk is importing (perhaps a few seconds), then `In [1]` when it's done and the kernel is ready to accept more commands. The \"`1`\" indicates it's the first cell to have been executed. Note that you can queue cells for execution, and they'll all show a `[*]` until they've executed."]},{"cell_type":"code","metadata":{"id":"Yww6r2sXjWfj"},"source":["import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yFc_42DfkPCx"},"source":["NLTK has a variety of text resources that can be used for examples and as training and test data for NLP models. There is a convenient application for downloading and accessing these resources: `nltk.download()`. When executed it launces a GUI interface that allows you to select and download these resources. Execute the following cell and click on the \"book\" collection then click \"download\". This collection contains all the resources used as teaching examples in the nltk book and in this notebook. Note that this need only be done once per installation. \n","\n","> __Beware__: the GUI will launch on the computer where the jupyter notebook server is running. If you are not running the server locally, running the following cell will not work as expected!"]},{"cell_type":"code","metadata":{"id":"PVvLhK-KjYDk"},"source":["nltk.download('book')\n","\n","# It is better to download all the nltk package in order not to encounter any \n","# errors related to missing nltk packages run the following commands \n","# but this usualy takes a while. You can do that using the following command:\n","# nltk.download('all')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VLcIsuktkc4n"},"source":["Now that the resources have been downloaded, you can access them as in the following cell. Each of the texts is an object with class `Text` - we'll see more about the functionalities of that class later, or you can have a look at the class in the [nltk documentation](http://www.nltk.org/api/nltk.html?highlight=text#nltk.text.Text)."]},{"cell_type":"code","metadata":{"id":"sw2FStsNkKdV"},"source":["from nltk.book import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AOTxZ7z6orSW"},"source":["Simply typing the name of a text will return its name. For instance the first text is **Moby Dick** and the second text is **Sense and Sensibility**."]},{"cell_type":"code","metadata":{"id":"t8OdZCIPkfFj"},"source":["text1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VQQ--ZYAoxoy"},"source":["text2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sRWZfN76pJq1"},"source":["## 2.1 Searching Text\n","\n","---\n","\n","\n","There are many ways to investigate a text. The `concordance()` method looks up every occurrence of a word and prints out it's context in the text. The following cell prints a concordance on the word \"monstrous\" from text1, which is _Moby Dick_ (a famous american novel).\n","\n","> __Try this:__ Look a few other concordances, such as \"affection\" in _Sense and Sensibility_ for the word affection, using `text2.concordance(\"affection\")` or \"lived\" in _Genesis_ (`text3`). You could look at `text1`, the _Moby Dick by Herman Melville 1851_, to see examples of English going back to 1789, and search for words like nation, terror, god to see how these words have been used differently over time. We've also included `text5`, the _NPS Chat Corpus_: search this for unconventional words like im, ur, lol. (Note that this corpus is uncensored!)"]},{"cell_type":"code","metadata":{"id":"ZsoAfpjDo30J"},"source":["text1.concordance(\"monstrous\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ly-Fxru-pQ0w"},"source":["Another interesting method of `Text` objects is `similar()`. This lists words that appear in similar contexts to a word. Here the context is the words immediately before or after an appearance of the given word (eg: \"monstrous\"). By the distributional hypothesis, these words relate to the same ideas in the text as the searched word."]},{"cell_type":"code","metadata":{"id":"Uucss0kkpOre"},"source":["text1.similar(\"monstrous\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PK-loIppSNH"},"source":["text2.similar(\"monstrous\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rB-sicYnpWFz"},"source":["The `common_contexts()` method finds contexts of two words that match. For example \"very\" is the most similar to \"monstrous\" in `text2`. Looking at the common contexts, we can see that \"monstrous\" is being used as an adjective in a similar way to \"very\".\n","\n","Have a play with some other pairs of words."]},{"cell_type":"code","metadata":{"id":"THc1WCGipTUN"},"source":["text2.common_contexts([\"monstrous\", \"very\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"biM3DKBKq7a4"},"source":["## 2.2 Counting Vocabulary\n","This section investigates the vocabulary, or list of words used in a text, and introduces function definitions and some useful utilites and data types."]},{"cell_type":"markdown","metadata":{"id":"OJCP6lZIq_OI"},"source":["Texts in nltk are represented as a sequence of words and punctuation symbols, or \"tokens\". In python, a _sequence_ is a sequential collection of things that can be accessed by their integer index with a knwon length.\n","\n","To get the number of tokens (words and puncuation symbols) in a text, we can use the python builtin function `len()`:"]},{"cell_type":"code","metadata":{"id":"-X29-Y_pqhfN"},"source":["# The number of tokens in text1, Moby Dick\n","len(text1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usGYHnwprGmW"},"source":["To see the set of unique tokens in a text (called as the vocabulary), we can use the python set type. A `set` is an unordered collection of objects, all of which are distinct. Adding an object to a set that is already there doesn't change the set. In the cell below, we use the `sorted()` python builtin function, which producdes a sorted version of the vocabulary.\n","\n","The resulting list is fairly long, which can be inconvenient in a notebook. To reuce the space it uses, click in the left margin: it'll be put into a scroll box. You can double click to make it go away entirely or click to make it come back to full size."]},{"cell_type":"code","metadata":{"id":"1cKK-uDxrBEB"},"source":["# vocabulary of text3\n","sorted(set(text3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MM0hYr4craiN"},"source":["The `len()` builtin function can be applied to a set, returning the number of objects it contains."]},{"cell_type":"code","metadata":{"id":"QXcgL5VrrNZo"},"source":["# size of the vocabulary of text3\n","len(set(text3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1uoFPTBCrdtR"},"source":["Now we can calculate a texts lexical diversity (a measure of average token frequency)."]},{"cell_type":"code","metadata":{"id":"tsePlyuOrb9u"},"source":["# lexical diversity\n","len(set(text3))/ float(len(text3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xN2i7seArixz"},"source":["The `Text` class has a convenient `count()` method that counts the number of occurences of a token. That allows us to easily calculate the frequency of a word in a text.\n","\n","> __Try This:__ Find the frequency of \"lol\" in text5."]},{"cell_type":"code","metadata":{"id":"YPi5iBNvrfWG"},"source":["# word frequency\n","text1.count('a') / float(len(text1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iEANvyw9rrQs"},"source":["Since we may want to calculate word frequency a lot for different pieces of text, we can make a function for it."]},{"cell_type":"code","metadata":{"id":"AgeihKaZrkO1"},"source":["def word_frequency(word, text):\n","    return text.count(word)/ float(len(text))\n","\n","word_frequency('a',text1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NBOXb4WBr73i"},"source":["Similarly, we can create a function to calculate the lexical diversity."]},{"cell_type":"code","metadata":{"id":"-4qFCCjRr4UT"},"source":["def lexical_diversity(text):\n","    return len(set(text)) / float(len(text))\n","\n","lexical_diversity(text3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MIMXL69hslQ7"},"source":["## 3. Frequency Distributions\n","\n","Let's say we want to identify the words of a text that are most informative about the topic and genre of the text? A good place to start would be to find the 50 most frequent words in the text. One method would be to keep a tally for each vocabulary item, something which we did in Exercise 0. \n","\n","A table with the counts of words for a book or any piece of text is called as the **Frequency Distribution**. It is a *distribution* because it tells us how the total number of word tokens in the text are distributed across the vocabulary items. Since we often need frequency distributions in language processing, NLTK provides built-in support for them."]},{"cell_type":"code","metadata":{"id":"w18AqtjQr7Ky"},"source":["# find the frequency distribution for Moby Dick\n","fdist1 = FreqDist(text1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YTIcjGh3tfVP"},"source":["The above cell found the frequency distribution and saved it in the variable `fdist`. We can find the 50 most frequent words by calling the `most_common` method."]},{"cell_type":"code","metadata":{"id":"y9136cHwtbhO"},"source":["fdist1.most_common(50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_uoZAOG5uuXD"},"source":["The 50 most common words are not very informative about the topic of the text. Only one word, whale, which occurs over 900 times is slightly informative. The rest of the words tell us nothing about the text; they're just English \"plumbing.\""]},{"cell_type":"markdown","metadata":{"id":"cVzJxiT4vUXz"},"source":["We can find what proportion of the text is taken up with such words? To do that we will make a cumulative frequency plot for these words, using the `fdist1.plot()` method."]},{"cell_type":"code","metadata":{"id":"00GC0JR5tuCs"},"source":["fdist1.plot(50, cumulative=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oTfPtToovfSf"},"source":["These 50 words account for more than 120,000 words out of the total 260,819 words, which is nearly half of the book.\n","\n","We can also look at the words which occur only once in the text. Such words are called **hapaxes** and we can see them by calling the `fdist1.hapaxes()` method."]},{"cell_type":"code","metadata":{"id":"fC2uzpE6uToo"},"source":["fdist1.hapaxes()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2gRQxkYSwKOc"},"source":["len(fdist1.hapaxes())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tVWLPRrIwRZa"},"source":["There are too many words (over 9000) which occur only once in the text and again these are not too useful. Perhaps, we can look at longer words and see if they characteristic or informative of the text."]},{"cell_type":"code","metadata":{"id":"i02XKLrSwQeU"},"source":["# first create vocabulary for the text\n","vocab = set(text1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_5SJEgAWw_KS"},"source":["We can look at the words which are 15 or more characters in length. \n","\n","This can be done by iterating over all the words in the vocabulary and checking if they satisfy the criteria of having the length of 15 or more characters."]},{"cell_type":"code","metadata":{"id":"RX1IrfdSw8MO"},"source":["long_words = []\n","\n","for word in vocab:\n","  if len(word) >= 15:\n","    long_words.append(word)\n","\n","long_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0A5Kgtxxbb4"},"source":["A more concise way to do the same thing in Python is by using list comprehension."]},{"cell_type":"code","metadata":{"id":"8wufCKVQxYgs"},"source":["long_words = [word for word in vocab if len(word) >= 15]\n","long_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-1DN4tjvyPmU"},"source":["These long words are often hapaxes and may not typify the text accurately. Finally, for this exercise, we will look for those long words which also occur somewhat frequently.\n","\n","In the cell below, we will look at words which are longer than 7 characters and occur more than 10 times in the text."]},{"cell_type":"code","metadata":{"id":"n8JOqrgGxmT7"},"source":["[w for w in set(text1) if len(w) > 7 and fdist1[w] > 10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"clf6QVCEzDZF"},"source":["We can also find the count for these words using the `text1.count()` function."]},{"cell_type":"code","metadata":{"id":"OQWLYHEtx2qH"},"source":["long_frequent_words = [w for w in set(text1) if len(w) > 7 and fdist1[w] > 10]\n","counts = [(word, text1.count(word)) for word in long_frequent_words]\n","counts"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BMU2eDsGzxiA"},"source":["And finally we can sort these words in decreasing order by their counts."]},{"cell_type":"code","metadata":{"id":"3ZIn3rSSzK5u"},"source":["sorted(counts, key=lambda x: x[1], reverse=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xeg0Qfhr0A3p"},"source":["By looking at the words, we can guess that this text is related to sea and ships with words such as *mariners*, *swimming*, *shipmates*, *fishermen* and *overboard* occurring frequently."]},{"cell_type":"markdown","metadata":{"id":"seRR7Jgx07Ym"},"source":["---\n","#### Can you do the similar steps and find out what topic does `text4` cover?"]},{"cell_type":"code","metadata":{"id":"q7ajHFZ-zt5q"},"source":["# your code goes here\n","sorted([(w, text4.count(w)) for w in set(text4) if len(w) > 7 and fdist1[w] > 10], key=lambda x: x[1], reverse=True)[:50]"],"execution_count":null,"outputs":[]}]}