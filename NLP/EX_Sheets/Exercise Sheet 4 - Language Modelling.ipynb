{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise Sheet 4 - Language Modelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIjZ1nARYUZl"
      },
      "source": [
        "# Exercise Sheet 4 - Language Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcp-LZu8JrTv"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "In this lab we are going to:\n",
        "\n",
        "- Play around with text corpora <br>\n",
        "- Learn some statistics tricks in Python and NLTK <br>\n",
        "- Learn about language modelling <br>\n",
        "- Learn about n-grams <br>\n",
        "- Naive bayes as a lanugage model <br>\n",
        "- Hands-on data sparsity and smoothing techniques <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOvc51Tj2rcG"
      },
      "source": [
        "# setting the stage ;)\n",
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1hyTgjNJrTx"
      },
      "source": [
        "--------------------\n",
        "## Text Corpus: Statistics and Probability\n",
        "\n",
        "### Accessing the corpus\n",
        "Open a Python session and  obtain the <a href=\"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/brown.zip\">Brown corpus</a>, using NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSZIeWNbJrTy"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# You will need to import 'Brown' as follows:\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# read a list of the words in the Brown Corpus\n",
        "list_words = brown.words()\n",
        "\n",
        "# print the first 20 words\n",
        "print(list_words[0:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3bBC-AUJrT3"
      },
      "source": [
        "We can access the corpus as a list of words, or a list of sentences (where each sentence is itself just a list of words). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO0ur7YwJrT4"
      },
      "source": [
        "brown.sents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_WYZeRAJrT-"
      },
      "source": [
        "The Brown corpus consists of different categories. We can list the available categories as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45q9qzaoJrT_"
      },
      "source": [
        "brown.categories()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQDgKd0XJrUF"
      },
      "source": [
        "We can access the text of a certain category as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv_HJnmZJrUG"
      },
      "source": [
        "brown.words(categories='fiction')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ArCN7UKJrUK"
      },
      "source": [
        "**Exercise 1:**\n",
        "\n",
        "What is the frequency of the word (ignoring case) &lsquo;world&rsquo; in the news category in the Brown corpus?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viQf81KvuUD0"
      },
      "source": [
        "#your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhJYR0pJJrUL"
      },
      "source": [
        "### Frequency of Words\n",
        "\n",
        "We can easily get the frequency distribution of the words in a corpus as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSs2tyjQJrUM"
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "news_text = brown.words(categories='news')\n",
        "\n",
        "# the frequency of each vocabulary item in the text\n",
        "fd = FreqDist(news_text)\n",
        "\n",
        "# total number of samples\n",
        "print (fd.N()) \n",
        "\n",
        "# how many unique words does this corpus have\n",
        "print (fd.B())\n",
        "\n",
        "# Get a list of the top 10 words sorted by frequency\n",
        "print(fd.most_common(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pdql3VeJrUP"
      },
      "source": [
        "**Exercise 2:**\n",
        "In the Brown Corpus, in which category(s) of the  news, government and editorial categories, the word (ignoring case) &lsquo;world&rsquo; has the highest total frequency?\n",
        "* news\n",
        "* government\n",
        "* editorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJXlE5Jd4yeb"
      },
      "source": [
        "# your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDCOhIEjJrUQ"
      },
      "source": [
        "### Probabilities\n",
        "\n",
        "**Exercise 3:**\n",
        "Calculate probabilities (relative frequency) of all words for only __news__ category in Brown corpora.\n",
        "What is the probability of the words &lsquo;jury&rsquo; and &lsquo;government&rsquo;?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b0sVdz1JrUQ"
      },
      "source": [
        "--------------------\n",
        "## N-Grams\n",
        "\n",
        "The probabilisic Language Models (a.k.a n-gram LMs) are developed to construct the joint probability distribution of a sequence of words. Based on the Markov assumption, the process of predicting a word sequence is broken up into predicting one word at a time.\n",
        "\n",
        "We can extract unigrams, and bigrams from a corpus as follows:\n",
        "In this example, we are going to generate unigrams and bigrams from the novel *Emma* by Jane Austen from The Gutenberg Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNTcTp8CJrUR"
      },
      "source": [
        "#explore the gutenberg corpus\n",
        "nltk.corpus.gutenberg.fileids()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5TAD0DbJrUV"
      },
      "source": [
        "# get the text of the novel Emma by Jane Austen \n",
        "emma_words = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "emma = \" \".join(emma_words) \n",
        "emma\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIo9GuniJrUX"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = nltk.word_tokenize(emma)\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlbo1pbuJrUZ"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "#unigrams\n",
        "print (list(ngrams(word_tokenize(emma), 1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9huW3ZbOJrUg"
      },
      "source": [
        "#bigrams\n",
        "print (list(ngrams(word_tokenize(emma[:50]), 2)))\n",
        "\n",
        "#or simply\n",
        "print(list(nltk.bigrams(emma_words[:50])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5YS0yJSJrUk"
      },
      "source": [
        "from nltk.probability import ConditionalFreqDist\n",
        "\n",
        "#Make a conditional frequency distribution of all the bigrams in the novel Emma by Jane Austen from The Gutenberg Corpus\n",
        "bigrams = nltk.bigrams(emma_words)\n",
        "\n",
        "cfd = ConditionalFreqDist(bigrams)\n",
        "\n",
        "#get the most frequently used word after ‘fully’\n",
        "cfd['fully']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hOTRC_qJrUn"
      },
      "source": [
        "#same with 'good' but sort by freq\n",
        "cfd['good'].most_common(20) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV8HQOkVJrUq"
      },
      "source": [
        "**Exercise 4:**\n",
        "Write a function to find the most common phrases (trigrams) in the __fiction__ category of the brown corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQslngsr5Ee4"
      },
      "source": [
        "# your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN9KGYQKJrUr"
      },
      "source": [
        "--------------\n",
        "## Probabilistic modeling\n",
        "\n",
        "\n",
        "### Naïve Bayes\tas\ta\tLanguage\tModel\n",
        "Based on probabilities of words in only the news and fiction categories in the brown corpus, classify the phrase 'mysterious murder case' to one of these categories. \n",
        "\n",
        "You should implement Naive Bayes classifier using probabilities of each word:\n",
        "\n",
        "$P(fiction|mysterious\\ murder\\ case) \\propto P(mysterious|fiction) \\times P(murder|fiction) \\times P(case|fiction) \\times P(fiction)$\n",
        "where $P(news) = 0.5$ and $P(fiction) = 0.5$\n",
        "\n",
        "**Exercise 5:**\n",
        "Write a general purpose Naive Bayes classifier such as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrP-lL_RJrUr"
      },
      "source": [
        "# template code to be updated\n",
        "from random import random\n",
        "def calculate_probability(phrase, category):\n",
        "    return random() # TODO: change this\n",
        "\n",
        "def naive_bayes(phrase):\n",
        "    news_prob = calculate_probability(phrase, 'news')\n",
        "    fiction_prob = calculate_probability(phrase, 'fiction')\n",
        "    if news_prob > fiction_prob:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0s5z593JrUt"
      },
      "source": [
        "### Smoothing\n",
        "\n",
        "A simple n-gram model would give zero probability to all of the combination that were not encountered in the training corpus, i.e. it would most likely give zero probability to most of the out-of-sample test cases. This problem is known as data sparsity and the traditional solution to it is to use smoothing techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmmHVlVhJrUu"
      },
      "source": [
        "#### Example: bigram model\n",
        "\n",
        "Given Corpus:\n",
        "\n",
        "$JOHN\\ READ\\ MOBY\\ DICK$\n",
        "<br>\n",
        "$MARY\\ READ\\ A\\ DIFFERENT\\ BOOK$\n",
        "<br>\n",
        "$SHE\\ READ\\ A\\ BOOK\\ BY\\ CHER$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApFuZUHoJrUv"
      },
      "source": [
        "**Exercise 6 (Pen and Paper):**\n",
        "Calculate the probability of the sentence \"JOHN READ A BOOK\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLVGxezoJrUx"
      },
      "source": [
        "**Exercise 7 (Pen and Paper):**\n",
        "What is the $p(CHER\\ READ\\ A\\ BOOK)$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1rhGpirJrUx"
      },
      "source": [
        "### Add-one smoothing\n",
        "\n",
        "$p(w_i|w_{i-1}) = \\frac{1 + c(w_{i−1} w_i)} {\\sum_{w'_i} [1 + c(w_{i−1} w'_i)] }$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C8JB5idJrU1"
      },
      "source": [
        "**Exercise 8 (Pen and Paper):**\n",
        "Re-calculate the $p(JOHN\\ READ\\ A\\ BOOK)$ and $p(CHER\\ READ\\ A\\ BOOK)$ using add-one smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNHEz_NDJrU7"
      },
      "source": [
        "#### Other Smoothing methods include:\n",
        "- Additive smoothing\n",
        "- Good-Turing estimate\n",
        "- Jelinek-Mercer smoothing (interpolation)\n",
        "- Katz smoothing (backoff)\n",
        "- Witten-Bell smoothing\n",
        "- Absolute discounting\n",
        "- Kneser-Ney smoothing"
      ]
    }
  ]
}